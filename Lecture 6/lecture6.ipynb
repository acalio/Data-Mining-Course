{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6\n",
    "\n",
    "## Classification\n",
    "In this lecture we will investigate another common supervised learning problem, i.e., the classification problem.\n",
    "\n",
    "Unlike the previous lecture, where we had to predicto a real-valued function, this time we are asked to predict\n",
    "some categorical values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT_DIR,'data')\n",
    "IMG_DIR = os.path.join(PROJECT_ROOT_DIR, 'img')\n",
    "CHAPTER_ID = \"classification\"\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "The MNIST dataset is probably the most famous dataset in ML. It is a set of 70.000 small images of \n",
    "handwritten digits collected from high school studends and employees of the US Census Bureau.\n",
    "\n",
    "Each image is labeled with the digit it represents.\n",
    "\n",
    "Since it is a very famous dataset, sklearn provides some helper function for\n",
    "download it and store it in memory.\n",
    "\n",
    "**Warning**: `fetch_mldata()` is deprecated since Scikit-Learn 0.20. You should use `fetch_openml()` instead. However, it returns the unsorted MNIST dataset, whereas `fetch_mldata()` returned the dataset sorted by target (the training set and the test test were sorted separately). In general, this is fine, but if you want to get the exact same results as before, you need to sort the dataset using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_target(mnist):\n",
    "    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]\n",
    "    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]\n",
    "    mnist.data[:60000] = mnist.data[reorder_train]\n",
    "    mnist.target[:60000] = mnist.target[reorder_train]\n",
    "    mnist.data[60000:] = mnist.data[reorder_test + 60000]\n",
    "    mnist.target[60000:] = mnist.target[reorder_test + 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_mnist():\n",
    "    if not os.path.isdir(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "    download = False\n",
    "    if not os.listdir(DATA_DIR):\n",
    "        download = True\n",
    "        # empty directory download the file\n",
    "        try:\n",
    "            from sklearn.datasets import fetch_openml\n",
    "            mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "            mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
    "            sort_by_target(mnist) # fetch_openml() returns an unsorted dataset\n",
    "        except ImportError:\n",
    "            from sklearn.datasets import fetch_mldata\n",
    "            mnist = fetch_mldata('MNIST original')\n",
    "    mode = 'wb' if download else 'rb'\n",
    "    with open(os.path.join(DATA_DIR,'mnist.pickle'), mode) as f:\n",
    "        if download:\n",
    "            pickle.dump(mnist, f)\n",
    "        else:\n",
    "            mnist = pickle.load(f)\n",
    "    return mnist\n",
    "mnist = maybe_download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the object. \n",
    "\n",
    "It has a dict like structure. You can access to the data, stored\n",
    "as a numpy 2-dimensional array with ``mnist.data``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([k for k in mnist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the shape of your dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70.000 images with 784 features. Each feature represent the indensity of\n",
    "a pixel, it ranges from 0 to 255. \n",
    "Each row-vector of the dataframe actually corresponds to the rollout of a 28x28 image.\n",
    "\n",
    "You can plot some data by reshaping the vector into the original matrix and then you\n",
    "can use ``matplotlib.imshow``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[36000]\n",
    "\n",
    "def draw(sample, save_path = None):\n",
    "    some_digit_image = some_digit.reshape(28, 28)\n",
    "    plt.imshow(some_digit_image, cmap = mpl.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    if save_path is not None:\n",
    "        save_fig(os.path.join(IMG_DIR, save_path))\n",
    "\n",
    "draw(some_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "example_images = np.r_[X[:12000:600], X[13000:30600:600], X[30600:60000:590]]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait**. Remember the last lecture? What should you do with the dataset at this stage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Binary Classifier\n",
    "Before addressing the entire problem, let's focus on a tiny version of it.\n",
    "\n",
    "Imagine you are asked to train a *binary classifier*, which should be only capable of \n",
    "detecting 5s.\n",
    "\n",
    "Let's create the new target vectors for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "some_digit = X[36000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let’s pick a classifier and train it. A good place to start is with a **Stochastic\n",
    "Gradient Descent (SGD)** classifier, using Scikit-Learn’s SGDClassifier class. This clas‐\n",
    "sifier has the advantage of being capable of handling very large datasets efficiently.\n",
    "\n",
    "**Note**: a few hyperparameters will have a different default value in future versions of Scikit-Learn, so a warning is issued if you do not set them explicitly. This is why we set `max_iter=5` and `tol=-np.infty`, to get the same results as in the book, while avoiding the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# train it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(some_digit)\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures for Classification Problem.\n",
    "\n",
    "In lecture 5 we have seen two common way of measuring the performance of our problem,\n",
    "i.e., the RMSE and MAE.\n",
    " \n",
    "**Question**: How do they apply in this context? Do they appear to you as a reasonable choice?\n",
    "\n",
    "*Answer*: \n",
    "\n",
    "----\n",
    "\n",
    "## Measuring Accuracy with Cross-Validation\n",
    "First of all the accuracy is defined as the ration between the number of correct predictions \n",
    "and the number of samples taken into account. \n",
    "\n",
    "More formally:\n",
    "\n",
    "$$\n",
    "ACCURACY = \\frac{\\#\\: right predictions}{\\#\\: predictions}\n",
    "$$\n",
    "\n",
    "### Implementing Cross-Validation\n",
    "Occasionally you will need more control over the cross-validation process than what\n",
    "cross_val_score() and similar functions provide. In these cases, you can implement\n",
    "cross-validation yourself; it is actually fairly straightforward.\n",
    "\n",
    "The StratifiedKFold class performs stratified sampling \n",
    "to produce folds that contain a representative ratio of each class. \n",
    "\n",
    "At each iteration the code creates a clone of the classifier, trains that clone on the training folds, \n",
    "and makes predictions on the test fold. Then it counts the number of correct predictions and\n",
    "outputs the ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# stratified splitting of the training samples\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "# your code here\n",
    "# Hint1 : call split and iterate over the 3 folds created by skfold\n",
    "# Hint2 : clone the classifier\n",
    "# Hint3 : call fit and then predict\n",
    "# Hint4 : compute and pring the accuracy of the current classifier\n",
    "#for train_index, test_index in skfolds.split(X_train, y_train_5):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, for the lazier persons sklean provides an utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_scores = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "cross_val_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95%! Great, right?\n",
    "\n",
    "**Question**: Can we be staisfied by this result?\n",
    "\n",
    "Let's implement another classifier. \n",
    "A dummy classifier that always predict\n",
    "``False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: draw your conclusions. What did just happen?\n",
    "\n",
    "Accuracy is not the preferred performance measure for classifiers, \n",
    "in particular when you have skewed data, i.e., some classes are much more frequent than others.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "A much better way to evaluate the performance of a classifier is to look at the confu‐\n",
    "sion matrix.\n",
    "\n",
    "The general idea is to count the number of times instances of class A are\n",
    "classified as class B.\n",
    "\n",
    "For example, to know the number of times the classifier confused\n",
    "images of 5s with 3s, you would look in the 5 th row and 3 rd column of the confusion\n",
    "matrix.\n",
    "\n",
    "To compute the confusion matrix, of course sklearn helps you with some helper function.\n",
    "In this case we are going to use the ``confusion_matrix`` function form ``sklearn.metrics``.\n",
    "\n",
    "In order to use it you first need to have  set of predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cofusion_matrix(matrix, classes):\n",
    "    s = \"\\t\"+\"\\t\".join(classes)\n",
    "    for i, row in enumerate(matrix):\n",
    "        s += \"\\n{}\\t{}\".format(classes[i], \"\\t\".join([str(x) for x in row]))\n",
    "    return s\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(display_cofusion_matrix(confusion_matrix(y_train_5, y_train_pred), ['not-5', '5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents the actual class while the column represents the predicted class.\n",
    "\n",
    "Since we are dealing with binary classification, we can devise 4 categories:\n",
    "* True Positive - samples predicted as positive that are actually positive (right classification)\n",
    "* True Negative - samples predicted as negative that are actually negative (right classification)\n",
    "* False Positive - samples predicted as positive that are actually negative (wrong classification)\n",
    "* False Negative - samples predicted as negative that are actually positibe( wrong classification)\n",
    "\n",
    "A perfect binary classifier whould have only true negative and false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train_5, y_train_perfect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![img/confmatrix.png](img/confmatrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "The confusion matrix is the building block for two other measures:\n",
    "\n",
    "* Precision - it is given by: \n",
    "    $$PRECISION = \\frac{TP}{TP+FP}$$\n",
    "    \n",
    "    It measures how many times\n",
    "    prediction of a positive sample were actually positive.\n",
    "* Recall - it is given by: $$ RECALL = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "    It measures how many positve samples are captured from the set of all the positive samples.\n",
    "    \n",
    "Let's use these measure over your trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? The model is far from being perfect as the >90% precision suggested before.\n",
    "\n",
    "Precision and recall, combined togheter, denote the so called $F_1$ score denoted as:\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = 2 \\times \\frac{precision \\times recall}{precision + recall} = \\frac{TP}{TP + \\frac{FN+TP}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $F_1$ score favors classifiers that have similar precision and recall. \n",
    "\n",
    "**Question**: Giving the same importance to both precision and recall, is it always the most reasonable choice?\n",
    "\n",
    "*Answer*....\n",
    "\n",
    "**Question**: How are precision and recall correlated with each other?\n",
    "\n",
    "*Answer*....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Precision/Recall Tradeoff\n",
    "\n",
    "``SGDClassifier`` for each point makes a decision based on a decision function. This function\n",
    "assigne a score to each sample and the final decision is taken on the basis of wheter or not\n",
    "this score exceeds a certain threshold.\n",
    "\n",
    "**Question** To what extents this threshold is correlated with recall and precision ?\n",
    "\n",
    "*Answer*\n",
    "\n",
    "---\n",
    "Let's see what happen when you increase or decrease the threshold.\n",
    "\n",
    "Wioth the method ``decision_function`` you can access to the score assigned to \n",
    "the sample given as input by the your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play around with thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 200000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to obtain the scores instead of the classes of each data point you can call\n",
    "``cross_val_predict`` with parameter ``decision_function``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")\n",
    "y_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually setting the threshold you can use the ``precision_recall_curve`` function of sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "print(precisions.shape, recalls.shape, thresholds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.xlim([-700000, 700000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can simply select the threshold value that gives you the best precision/recall\n",
    "tradeoff for your task. Another way to select a good precision/recall tradeoff is to plot\n",
    "precision directly against recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores > 70000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to select a good tradeoff between precision and recall is to plotprecision directly against recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if someone presents to you a 90 percent precision classifier you should also ask \n",
    "for its recall. In fact, having high-precision classifier is  not very useful if the recall is too low!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves\n",
    "The receiver operating characteristic curve is another common tool used\n",
    "with binary classifier.\n",
    "\n",
    "Thge ROC curve plots the true positive rate (another name for Recall) against the false positive rate.\n",
    "This latter measure, FPR, is teh ratio of negative samples that are wrongly classified as positive.\n",
    "\n",
    "FPR is eaaul to 1 minus the true negative rate,which is the ratio of negative instances that are correcly classified as negative.\n",
    "\n",
    "The ROC curve plots the recall vs the 1-specificity.\n",
    "\n",
    "As a recap:\n",
    "* True positive rate (recall / sensitivity)\n",
    "    $$TPR = \\frac{TP}{TP+FP} = 1 - FNR$$\n",
    "\n",
    "* False positive rate \n",
    "    $$\n",
    "        FPR = \\frac{FP}{FP+TN} = 1 - TNR\n",
    "    $$\n",
    "* True negative rate (specificity)\n",
    "    $$\n",
    "        TNR = \\frac{TN}{TN+FP} = 1 - FPR\n",
    "    $$\n",
    "\n",
    "To plot the ROC curve, you first need to compute the TPR and FPR for various threshold values, using the  ``roc_curve`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the ROC curve suggests that there is a thadeoff. The higher the recall the more false positives are predicted. \n",
    "\n",
    "The dotted line represents the ROC curve associated to a purely random classifier. \n",
    "One may intuitively say that the performance of a model improves as the distance between the curves increases.\n",
    "\n",
    "Another way for comparing two calssifiers is to look at the area under the ROC curve. This metrics is called\n",
    "AUC. An ideal classifier will always have an area equal to 1. \n",
    "\n",
    "sklearn provides a function for computing this area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "You have seen a number of different way for measuring the performance of a classifier. \n",
    "\n",
    "You are probably wondering how to choose the correct one.\n",
    "As a general rule of thumb Precision Recall curve is preferred whenever the positive class is rare or when yo care omre about false positives than the false negatives. Otherwise you should prefer\n",
    "using the ROC curve or the AUC score as well.\n",
    "\n",
    "If you look to the AUC score, you see a value really close to 1. \n",
    "This result suggests that your model works pretty good. However, if you look at the PR curve you may see that\n",
    "there is still room for improvement, and the good performance are mostly due to fact that the \n",
    "dataset is strongly unbalanced towards the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Train a ``RandomForestClassifier`` (Hint: pass ``predict_proba`` as the ``method`` argument).\n",
    "\n",
    "Display the ROC curve, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#train it\n",
    "y_probas_forest = # get prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\n",
    "precision_score(y_train_5, y_train_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "So far we tackled just binary classification problem. There are otehr kind of\n",
    "classification problem, though. One of this is the *multiclass* calssification problem.\n",
    "\n",
    "Instead of having to choose between only a pair of labels, you need to assign a label amongs a number\n",
    "of them.\n",
    "\n",
    "Some algorithms, such the Random Forest Classifier, or the naive Bayes classifiers, are capable to handling\n",
    "multiple class directly.\n",
    "\n",
    "However, there are several other strategies, that allows you to extend the binary classification framework to\n",
    "this context.\n",
    "\n",
    "For instance, regarding the MNIST dataset, one may think of using several binary classifiers. \n",
    "This strategy can be implemented according to two main approaches:\n",
    "\n",
    "1. One vs All (OVA) - if you are asked to predict a class amongst N different ones, you can train N different binary classifiers. Each one of them is trained in order to detect a single class against all the others.\n",
    "2. One vs One (OVO) - given N different class, you can  N x (N-1)/2 different binary classifier. You will need to train a classifier for every possible pair of classes.\n",
    "\n",
    "For most binary classification algorithms the OVA approach is the one to be preferred.\n",
    "\n",
    "sklearn authomatically detects when you need to solve a multiclass classification problem.\n",
    "In the following cell we will train the same classifier defined above; this time we are going to pass to the \n",
    "``fit`` function the entire training targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood,\n",
    "Scikit-Learn actually trained 10 binary classifiers, got their decision scores for the\n",
    "image, and selected the class with the highest score.\n",
    "\n",
    "To convince yourself about the actual strategy adopted by sklearn you can \n",
    "compute the scores associated with the ``some_digit``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "some_digit_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each score ~~represents~~ **reflects** the likelihood of the given sample to belong to the corresponding class.\n",
    "\n",
    "The class is assigned computing the argmax on this vector of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(some_digit_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Be careful, this 5 actually is the index of the maximum element, it is not its class.\n",
    "\n",
    "The classess upon which the classifier is trained can be accessed via the ``classes_`` field of the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.classes_[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to force sklearn to use the opposite strategy, OVO, you can use the\n",
    "``OneVsOneClassifier``.\n",
    "You need to pass a regular binary classifier and then call the usual fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "ovo_clf = OneVsOneClassifier(SGDClassifier(max_iter=5, tol=-np.infty, random_state=42))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "ovo_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ovo_clf.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with the **RandomFrestClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.predict_proba([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets over 84% on all test folds. If you used a random classifier, you would get 10%\n",
    "accuracy, so this is not such a bad score, but you can still do much better.\n",
    "\n",
    "We have seen that scaling the data usually helps the training process.\n",
    "Now, I ask you to scale the data and then train the same RandomForestClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "#your code here\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Here, we will assume that you have found a promising model and\n",
    "you want to find ways to improve it. One way to do this is to analyze the types of\n",
    "errors it makes.\n",
    "\n",
    "You need to make predictions using the ``cross_val_predict`` fnction and then\n",
    "call the ``confusion_matrix()`` function, just like\n",
    "you did earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_confusion_matrix(matrix):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.heatmap(matrix, \n",
    "                annot=False,  \n",
    "                cmap=\"YlGnBu\")\n",
    "            \n",
    "plot_confusion_matrix(conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s focus the plot on the errors. First, you need to divide each value in the confusion\n",
    "matrix by the number of images in the corresponding class, so you can compare error\n",
    "rates instead of absolute number of errors (which would make abundant classes look\n",
    "unfairly bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns for classes 8 and 9 are quite bright, which tells you that many images get misclassified as\n",
    "8s or 9s. \n",
    "\n",
    "Similarly, the rows for classes 8 and 9 are also quite bright, telling you that 8s\n",
    "and 9s are often confused with other digits. \n",
    "\n",
    "Conversely, some rows are pretty dark,\n",
    "such as ow 1: this means that most 1s are classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel classification\n",
    "\n",
    "Until now each instance has always been assigned to just one class. In some cases you\n",
    "may want your classifier to output multiple classes for each instance. For example,\n",
    "consider a face-recognition classifier: what should it do if it recognizes several people\n",
    "on the same picture?\n",
    "\n",
    "Such a classification system that outputs multiple\n",
    "binary labels is called a multilabel classification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a y_multilabel array containing two target labels for each digit\n",
    "image: the first indicates whether or not\n",
    "the digit is large (7, 8, or 9) and the second\n",
    "indicates whether or not it is odd. The next lines create a KNeighborsClassifier,\n",
    "Now you can make a prediction, and notice\n",
    "that it outputs two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\n",
    "\n",
    "There are many ways to evaluate a multilabel classifier, and selecting the right metric\n",
    "really depends on your project. For example, one approach is to measure the F 1 score\n",
    "for each individual label (or any other binary classifier metric discussed earlier), then\n",
    "simply compute the average score. This code computes the average F 1 score across all\n",
    "labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3, n_jobs=-1)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumes that all labels are equally important, which may not be the case.\n",
    "\n",
    "One simple option is to give each label a weight equal to its support (i.e., the number of instances with that\n",
    "target label). To do this, simply set average=\"weighted\" in the preceding code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. An MNIST Classifier With Over 97% Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN classifier appears to be one with the best performance.\n",
    "Try to push its performance with automatica parameter tuning.\n",
    "It has only one parameter to tune: k, it represents the number of neighbors to take into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing and Selecting the best Model\n",
    "Now, you are required to build an entire pipeline. \n",
    "You need to select a number of models, tuning upon each of them and then you need to report in a table the following\n",
    "information:\n",
    "\n",
    "1. Name of The estimator\n",
    "2. Best Configuration\n",
    "3. accuracy\n",
    "3. precision\n",
    "4. recall\n",
    "5. AUC score\n",
    "\n",
    "Of course, these results must be obtained wrt the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
