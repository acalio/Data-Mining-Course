{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining, Lecture 3\n",
    "\n",
    "## Handling data with Pandas\n",
    "\n",
    "pandas contains high-level data structures and manipulation tools designed to make data\n",
    "analysis fast and easy in Python. \n",
    "\n",
    "pandas is built on top of NumPy and makes it easy to\n",
    "use in NumPy-oriented applications.\n",
    "\n",
    "Here are some of the major features pandas provides:\n",
    "\n",
    "* Data structures with labeled axes supporting automatic or explicit data alignment.\n",
    "  This prevents common errors resulting from misaligned data and working with\n",
    "  differently-indexed data coming from different sources.\n",
    "* Integrated time series functionality\n",
    "* The same data structures handle both time series data and non-time series data.\n",
    "* Arithmetic operations and reductions (like summing across an axis) would pass\n",
    "  on the metadata (axis labels)\n",
    "* Flexible handling of missing data.\n",
    "* Merge and other relational operations found in popular database databases (SQL-\n",
    "  based, for example).\n",
    "---\n",
    "\n",
    "Let's start and import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series\n",
    "from io import StringIO \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from IPython.display import Image\n",
    "IMG_PATH, DATA_PATH = \"./img\", \"./data\"\n",
    "\n",
    "\n",
    "def describe(a):\n",
    "    if type(a) is np.ndarray:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ndtype:{}\\ntype: {}\".format(a, a.shape, a.dtype, type(a)))\n",
    "    elif type(a) is pd.Series:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ndtype:{}\\nname:{}\\nindex-name:{}\\ntype:{}\".format(a, a.shape, a.dtype, a.name, a.index.name, type(a)))\n",
    "    elif type(a) is pd.DataFrame:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ntype:{}\".format(a, a.shape,type(a)))\n",
    "    else:\n",
    "        print(\"{}, type:{}\".format(a, type(a)))\n",
    "\n",
    "\n",
    "hrule = lambda x : \"=\"*x\n",
    "Hrule = lambda x,y: \"=\"*(x//2)+y+\"=\"*(x//2)\n",
    "Data = lambda file : os.path.join(DATA_PATH, file)\n",
    "Img  = lambda img : os.path.join(IMG_PATH, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to pandas Data Structures\n",
    "\n",
    "To get started with pandas, you will need to get comfortable with its two workhorse\n",
    "data structures: Series and DataFrame.  ---- riscrivere ----\n",
    "\n",
    "### Series\n",
    "A Series is a one-dimensional array-like object containing an array of data (of any\n",
    "NumPy data type) and an associated array of data labels, called its index. \n",
    "\n",
    "The simplest example of pandas Series is just a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series(np.arange(10))\n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**: Likewise vanilla numpy array, a Series object also has a dtype and a shape.\n",
    "\n",
    "---\n",
    "When we print a Series object, the index associated with each data cotained in the\n",
    "Series is also printed on the leftside.\n",
    "\n",
    "Since in the previous case we did not specify andy index, by default pandas assign\n",
    "all the integers within the interval $0$ and $N-1$ ($N$ is the length of the object).\n",
    "\n",
    "We can also separately access to index ad values of a Series object, as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj.index, obj.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it will be desirable to create a Series with an index identifying each data point.\n",
    "You can specify the index during the Series creation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series([1,2,3,4], index=list('abcd'))\n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with a regular NumPy array, you can use values in the index when selecting\n",
    "single values or a set of values. \n",
    "\n",
    "Likewise NumPy, a Series object accept all the indexing strategies\n",
    "provided by a regular array.\n",
    "\n",
    "---\n",
    "\n",
    "**Excercise** - Get Familiar with Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A new Series from obj.\n",
    "    Result\n",
    "    -----\n",
    "    values = [4,4,4,7,3]\n",
    "\n",
    "Hint: Fancy indexing\n",
    "''' \n",
    "# your code here \n",
    "print(hrule(20))\n",
    "'''\n",
    "A series containing only the\n",
    "positive values\n",
    "    Result\n",
    "    -----\n",
    "    values = [4,7,3]\n",
    "''' \n",
    "\n",
    "#your code here\n",
    "print(hrule(20))\n",
    "\n",
    "'''\n",
    "    Result\n",
    "    ------\n",
    "    values = [16, 49, 25, 9]\n",
    "'''\n",
    "\n",
    "# your code here\n",
    "print(hrule(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "It should be noted that a Series looks like a fixed-length, ordered dict, \n",
    "as it is a mapping of index values to data values. \n",
    "\n",
    "As a result, it can be substituted into many functions that expect a\n",
    "dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obj = Series(\n",
    "    [1,2,3,4,5,6,7],\n",
    "    index=np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe']),\n",
    ")\n",
    "\n",
    "# Note: 1 key multiple values\n",
    "print(obj['Joe']) # access like a dict \n",
    "print(hrule(20))\n",
    "\n",
    "print(obj.Joe)  # access as an object field\n",
    "print(hrule(20))\n",
    "\n",
    "# test membership\n",
    "print('Joe' in obj)\n",
    "print('Frank' in obj)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the index is not unique. Several values can be associated with the same index\n",
    "\n",
    "---\n",
    "It is also straightforward to construct a Series from a Python dict.\n",
    "You just need to pass the dict to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n",
    "obj3 = Series(sdata)\n",
    "print(obj3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the index is ordered by default, unless we specify the exact order of the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['California', 'Ohio', 'Oregon', 'Texas']\n",
    "\n",
    "obj = Series(sdata, index=states) \n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: California is not included in the original dict sdata. For this reason in the Series object appears a NaN corresponding to the *California* index\n",
    "\n",
    "---\n",
    "\n",
    "We can obtain a mask for selecting all the (not)null values from the Series as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(obj.isnull()) # equivalently pd.isnull(obj)\n",
    "print(hrule(20))\n",
    "\n",
    "describe(pd.notnull(obj)) # equivalently obj.notnull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Series also allow the user to specify a name to be associted both to the Series object itself and the correponding index.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.name = \"population\"\n",
    "obj.index.name = \"states\"\n",
    "describe(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series’s index can be altered in place by assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series(\n",
    "    [1,2,3,4,5,6,7],\n",
    "    index=np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe']),\n",
    ")\n",
    "obj.index = ['Bob', 'Bob', 'Joe', 'Joe', 'Will', 'Joe', 'Will']\n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "A DataFrame represents a tabular, spreadsheet-like data structure containing an or-\n",
    "dered collection of columns, each of which can be a different value type (numeric,\n",
    "string, boolean, etc.). The DataFrame has both a row and column index; it can be\n",
    "thought of as a dict of Series (one for all sharing the same index).\n",
    "\n",
    "---\n",
    "\n",
    "There are numerous ways to construct a DataFrame, though one of the most common\n",
    "is from a dict of equal-length lists or NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each key will correspond to a column of the DataFrame\n",
    "data = {\n",
    "    'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n",
    "    'year': [2000, 2001, 2002, 2001, 2002],\n",
    "    'pop': [1.5, 1.7, 3.6, 2.4, 2.9]\n",
    "}\n",
    "df = DataFrame(data)  # Note: the index is automatically assigned \n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rearrange the columns of the dataframe by explictly defining the exact order in which you want them to appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(data, columns=['year', 'state', 'pop'])\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise the Series, passing a column that isn’t contained in data will lead to a column of NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(df, columns=['year', 'state', 'pop', 'dept'])\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing a DataFrame\n",
    "\n",
    "### Accessing the column\n",
    "You can directly access to a column of the DataFrame using a dict-like or attribute-like notation. \n",
    "Pandas will return a Series object **whose index corresponds to the one in the original dataframe**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df.state)\n",
    "print(hrule(20))\n",
    "describe(df['state']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column indexing can also be used for adding a new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['density'] = 10 # broadcast a scalar value\n",
    "describe(df)\n",
    "print(hrule(30))\n",
    "\n",
    "df['density'] = np.arange(df.shape[0]) # we can also use a ndarray\n",
    "describe(df)\n",
    "print(hrule(30))\n",
    "\n",
    "df['density'] = Series(np.arange(df.shape[0])) # Pay Attention\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When assigning lists or arrays to a column the length must match. If you assigne a Series, the index of the DataFrame and and the Series\n",
    "must match with each other, otherwise Pandas will set a NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Excercise** - Look at those NAN. Please, figure it out a solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "------\n",
    "  year   state  pop  density\n",
    "a  2000    Ohio  1.5        0\n",
    "b  2001    Ohio  1.7        1\n",
    "c  2002    Ohio  3.6        2\n",
    "d  2001  Nevada  2.4        3\n",
    "e  2002  Nevada  2.9        4\n",
    "\n",
    "'''\n",
    "df['density'] = Series(np.arange(df.shape[0]), df.index)   # your code here\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the Series object returned is actually a *view* on the original data, thus any change to that object will be reflected\n",
    "on the dataframe (unless we call copy() the Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'][0] = 'New York'\n",
    "describe(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Wait a minute! What the heck is this Warning?**\n",
    "\n",
    "Firt of all, it is just a warning and not an error. \n",
    "While an error indicates that something is broken, \n",
    "such as invalid syntax or an attempt to reference an undefined variable, \n",
    "the job of a warning is to alert the programmer to potential bugs or issues with their \n",
    "code that are still permitted operations within the language. \n",
    "\n",
    "\n",
    "*SettingWithCopyWarning* informs you that\n",
    "your operation might not have worked as expected and that you \n",
    "should check the result to make sure you haven’t made a mistake.\n",
    "\n",
    "To understand what SettingWithCopyWarning is about, \n",
    "it’s helpful to understand that some actions in pandas can return a view of your data, \n",
    "and others will return a copy.\n",
    "\n",
    "Therefore this Warning is kindly suggesting to check whether\n",
    "the intrunction you issued has worked as you expected to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(os.path.join(IMG_PATH, 'viewvscopy.png'), width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Accessing via iloc vs. loc\n",
    "Let's create another DataFrame and try to access to the first row with a dict-like notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(data, columns=['year', 'state', 'pop'], index=np.flip(np.arange(0, len(next(iter(data)))))) # LOOK: the index is in reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0] #ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access to a row of the dataframe you must use one of the two following functions:\n",
    "\n",
    "**iloc**\n",
    "\n",
    "iloc gets rows (or columns) at particular positions in the index (so it takes integers or boolean arrays)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(data, columns=['year', 'state', 'pop'], index=np.flip(np.arange(0, len(next(iter(data)))))) # LOOK: the index is in reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df)\n",
    "print(hrule(20))\n",
    "\n",
    "print(\"index: {}\".format(frame.index))\n",
    "\n",
    "\n",
    "print(Hrule(20, \"First Element\"))\n",
    "print(df.iloc[0]) \n",
    "\n",
    "print(Hrule(20, \"Second Element\"))\n",
    "print(df.iloc[1])\n",
    "\n",
    "print(Hrule(20, \"Element with index 4\"))\n",
    "print(df.loc[4])\n",
    "\n",
    "\n",
    "print(Hrule(20, \"state value of element with index 3\"))\n",
    "print(df.iloc[1]['state'])\n",
    "print(hrule(5))\n",
    "print(df.iloc[1, 1]) # 1 because 'state' is the column with index 1\n",
    "print(df.iloc[1].state) # accessing a field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**loc**\n",
    "\n",
    "gets rows (or columns) with particular labels from the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = DataFrame(data, columns=['year', 'state', 'pop'], index=[chr(x) for x in range(97, 97+len(next(iter(data))))]) # LOOK: the index is in reverse order\n",
    "describe(df_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Hrule(20, \"Element with index a\"))\n",
    "print(df_.loc['a'])\n",
    "\n",
    "print(Hrule(20, \"Element with index b, year\"))\n",
    "print(df_.loc['a']['pop']) \n",
    "print(hrule(5))\n",
    "print(df_.loc['a', 'pop']) # the same as above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Exercise**\n",
    "\n",
    "Get familiar with dataframe indexing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(data, columns=['year', 'state', 'pop'], index=[chr(x) for x in range(97, 97+len(next(iter(data))))])\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Result\n",
    "------\n",
    "    a    2000\n",
    "    b    2001\n",
    "    c    2002\n",
    "    d    2001\n",
    "    e    2002\n",
    "Requirement: Dict-like access\n",
    "''' \n",
    "describe(\n",
    "    # your code here \n",
    ")\n",
    "print(hrule(20))\n",
    "'''\n",
    "Result\n",
    "-----\n",
    "    'Ohio' of the second element\n",
    "Requirement: Object-like access + dict-like access\n",
    "''' \n",
    "describe(\n",
    "    #your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "'''\n",
    "Result\n",
    "------\n",
    "    year       2000\n",
    "    state      Ohio\n",
    "    pop         1.5\n",
    "    density       0\n",
    "'''\n",
    "describe(\n",
    "    # your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "Result\n",
    "------\n",
    "    year    2000\n",
    "    pop      1.5\n",
    "    Name: a\n",
    "Hint: Fancy Indexing\n",
    "'''\n",
    "describe(\n",
    "    # your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "Result\n",
    "------\n",
    "  array([2001, 'Nevada', 2.4], dtype=object)\n",
    "Hint: .values\n",
    "'''\n",
    "describe(\n",
    "    # your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "Result\n",
    "------\n",
    "     array([2001, 2.4], dtype=object) (the fourth element)\n",
    "Hint: .values\n",
    "'''\n",
    "describe(\n",
    "    # your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "Result\n",
    "------\n",
    "    array([[2000, 'Ohio', 1.5],\n",
    "       [2001, 'Ohio', 1.7],\n",
    "       [2002, 'Ohio', 3.6],\n",
    "       [2001, 'Nevada', 2.4],\n",
    "       [2002, 'Nevada', 2.9]], dtype=object)\n",
    "\n",
    "The entire dataframe as a ndarray\n",
    "'''\n",
    "describe(\n",
    "    # your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Objects\n",
    "\n",
    "pandas’s Index objects are responsible for holding the axis labels and other metadata\n",
    "(like the axis name or names). Any array or other sequence of labels used when con-\n",
    "structing a Series or DataFrame is internally converted to an Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series(range(3), index=['a', 'b', 'c'])\n",
    "index = obj.index\n",
    "\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index objects are immutable and thus can’t be modified by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index[1] = 'c' # ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame is associated with an index for each axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index) # row index\n",
    "print(df.columns) # column index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following table contains the Main Index objects in pandas\n",
    "\n",
    "|Class|Description|\n",
    "|:-:|:-:|\n",
    "|Index |The most general Index object, representing axis labels in a NumPy array of Python objects. |\n",
    "|Int64Index | Specialized Index for integer values. |\n",
    "|MultiIndex | “Hierarchical” index object representing multiple levels of indexing on a single axis. Can be thought of as similar to an array of tuples.|\n",
    "|DatetimeIndex | Stores nanosecond timestamps (represented using NumPy’s datetime64 dtype).|\n",
    "|PeriodIndex | Specialized Index for Period data (timespans).|\n",
    "\n",
    "\n",
    "The following table contains a list useful index methods\n",
    "\n",
    "|Method| Description|\n",
    "|:-:|:-:|\n",
    "|append | Concatenate with additional Index objects, producing a new Index|\n",
    "|diff |Compute set difference as an Index|\n",
    "|intersection |Compute set intersection|\n",
    "|union |Compute set union|\n",
    "|isin |Compute boolean array indicating whether each value is contained in the passed collection|\n",
    "|delete| Compute new Index with element at index i deleted|\n",
    "|drop |Compute new index by deleting passed values|\n",
    "|insert| Compute new Index by inserting element at index i|\n",
    "|is_monotonic| Returns True if each element is greater than or equal to the previous element|\n",
    "|is_unique| Returns True if the Index has no duplicate values|\n",
    "|unique |Compute the array of unique values in the Index|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential Functionality\n",
    "A list of fundamental mechanics of interacting with\n",
    "the data contained in a Series or DataFrame\n",
    "\n",
    "### Reindexing\n",
    "Create a new object\n",
    "with the data conformed to a new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])\n",
    "describe(obj)\n",
    "print(Hrule(20, \"Reindexing\"))\n",
    "obj2 = obj.reindex(['a','a', 'b','b', 'd','c','d'])\n",
    "describe(obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The element of the original object are rearrenged according to the list passed to the method reindex.\n",
    "\n",
    "We can also insert new index-element with default values. In such scenario pandas will\n",
    "insert the default value iff teh new index is not in the original Series object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = obj.reindex(['a', 'z']) # NaN as defualt behavior\n",
    "describe(obj2)\n",
    "print(hrule(20))\n",
    "obj2 = obj.reindex(['a', 'z'], fill_value = -1) # full with -1\n",
    "describe(obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "reindex works with pandas dataframe as well. It alway return a\n",
    "copy of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])\n",
    "describe(df)\n",
    "print(Hrule(20, \"Reindexing rows\"))\n",
    "df_ = df.reindex(['a', 'b', 'c', 'd']) # reindexing rows\n",
    "describe(df_)\n",
    "print(Hrule(20, \"Reindexing columns\"))\n",
    "df_ = df.reindex(columns=['Texas', 'Ohio', 'Ohio'])\n",
    "describe(df_)\n",
    "print(Hrule(20, \"Reindexing rows & columns\"))\n",
    "df_ = df.reindex(index=['a','a','b'], columns=['Texas', 'Ohio', 'Ohio'])\n",
    "describe(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping entries from an axis\n",
    "Dropping one or more entries from an axis is easy if you have an index array or list\n",
    "without those entries. As that can require a bit of munging and set logic, the drop\n",
    "**method will return a new object** with the indicated value or values deleted from an axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(np.arange(16).reshape((4, 4)),\n",
    "                index=['Ohio', 'Colorado', 'Utah', 'New York'],\n",
    "                columns=['one', 'two', 'three', 'four'])\n",
    "\n",
    "df_ = df.drop(['Colorado', 'Utah']) #dropping rows\n",
    "describe(df_)\n",
    "print(hrule(20))\n",
    "df_ = df.drop(['one','four'], axis=1) # dropping columns\n",
    "describe(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, selection, and filtering\n",
    "\n",
    "It is very similar to numpy, therefore we can user slicing, fancy indexing and boolean indexing (See Lecture 2).\n",
    "\n",
    "---\n",
    "**Exercises**: Adopt and see how the indexing techniques of numpy ndarray apply to Series and DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series(np.arange(4.), index=reversed(['a', 'b', 'c', 'd']))\n",
    "df =DataFrame(np.arange(16).reshape((4, 4)),\n",
    "              index=['Ohio', 'Colorado', 'Utah', 'New York'],\n",
    "              columns=['one', 'two', 'three', 'four'])\n",
    "print(Hrule(20, \"Series\"))\n",
    "describe(obj)\n",
    "\n",
    "print(Hrule(20, \"DataFrame\"))\n",
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The first element of the series (positional indexing)\n",
    "Result\n",
    "------\n",
    "    0\n",
    "'''\n",
    "describe(\n",
    "    #your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "'''\n",
    "The d-element of the series\n",
    "Result\n",
    "-----\n",
    "    0\n",
    "'''\n",
    "describe(\n",
    "    #your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "All the elements within d and c\n",
    "Result\n",
    "------\n",
    "    d 0\n",
    "    c 1\n",
    "\n",
    "Hint: slice operator    \n",
    "'''\n",
    "describe(\n",
    "    # your code here \n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "'''\n",
    "Return all the elements less or equal to 2\n",
    "Result\n",
    "------\n",
    "    a    0.0\n",
    "    b    1.0\n",
    "''' \n",
    "describe(\n",
    "    # your code here \n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "'''\n",
    "Set all the elements between\n",
    "the firsts and the last-1 element\n",
    "of obj to -1\n",
    "'''\n",
    "describe(\n",
    "    # your code here \n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "'''\n",
    "Select the first two rows\n",
    "of the dataframe\n",
    "'''\n",
    "describe(\n",
    "    # your code here \n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "Select all the element of\n",
    "the dataframe with value greater\n",
    "than 3 in column 'three'\n",
    "\n",
    "Requirements: Give me at least two different ways\n",
    "\n",
    "Hints: use a mask for the first method.\n",
    "       Combine boolean indexing on the rows with the iloc function\n",
    "'''\n",
    "describe(\n",
    "    # your code here \n",
    ")\n",
    "print(hrule(20))\n",
    "describe(\n",
    "    # your code here \n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "As before, all the columns except\n",
    "the last one\n",
    "'''\n",
    "describe(\n",
    "    # your code here \n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "\n",
    "'''\n",
    "As before, all the rows until\n",
    "\"Utah\". All the columns within\n",
    "\"two\" and \"three\".\n",
    "\n",
    "Hint: use loc function\n",
    "'''\n",
    "describe(\n",
    "    # your code here \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Arithmetic and data alignment\n",
    "There is only one important aspect that you need to be aware of:\n",
    "the behavior of arithmetic between ob-\n",
    "jects with different indexes. \n",
    "\n",
    "When adding together objects, if any index pairs are not\n",
    "the same, the respective index in the result will be the union of the index pairs. Let’s\n",
    "look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])\n",
    "s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])\n",
    "describe(s1+s2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames have the same behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])\n",
    "df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "\n",
    "describe(df1 + df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can avoid this ugly NaN values by specifying a fill_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the NaN corresponding to index mismatching\n",
    "tmp = df1.add(df2, fill_value=0)\n",
    "describe(tmp) \n",
    "print(hrule(30))\n",
    "# fill the NaN corresponding to missing columns\n",
    "tmp_columns = df2.columns.union(df1.columns)   # set operations between index\n",
    "tmp = df1.reindex(columns=tmp_columns, fill_value=0)\n",
    "describe(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations between DataFrame and Series\n",
    "By default, arithmetic between DataFrame and Series matches the index of the Series\n",
    "on the DataFrame's columns, broadcasting down the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "series = frame.iloc[0]\n",
    "\n",
    "describe(frame)\n",
    "print(hrule(20))\n",
    "\n",
    "describe(series)\n",
    "print(hrule(20))\n",
    "\n",
    "describe(frame-series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function application and mapping\n",
    "\n",
    "NumPy ufuncs (element-wise array methods) work fine with pandas objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "describe(frame)\n",
    "print(hrule(50))\n",
    "describe(np.abs(frame))\n",
    "print(hrule(50))\n",
    "f = lambda x :  x.max() - x.min()\n",
    "describe(frame.apply(f, axis=0)) # compute f over each column\n",
    "print(hrule(50))\n",
    "describe(frame.apply(f, axis=1)) # compute f over each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Many of the most common array statistics (like sum and mean ) are DataFrame methods,\n",
    "so using apply is not necessary.\n",
    "\n",
    "The function passed to apply need not return a scalar value, it can also return a Series or a ndarray\n",
    "with multiple values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return Series([x.min(), x.max()], index=['min', 'max'])\n",
    "\n",
    "def f_(x):\n",
    "    return [x.min(), x.max()] # it is better to use a Series\n",
    "\n",
    "frame = DataFrame(np.random.randint(0,100,12).reshape(4,3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "describe(frame)\n",
    "print(hrule(20))\n",
    "\n",
    "describe(frame.apply(f))\n",
    "print(hrule(50))\n",
    "describe(frame.apply(f, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have applied function axis-wise. It is not the only way,\n",
    "element-wise Python functions can be used, too. Suppose you wanted to compute a\n",
    "formatted string from each floating point value in frame . You can do this with **map**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise**\n",
    "\n",
    "Get familiar with element-wise functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow2 = #compute the square \n",
    "sqrt = # compute the square root\n",
    "\n",
    "'''\n",
    "Apply pow2 element-wise over frame\n",
    "'''\n",
    "describe(\n",
    "    #your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "'''\n",
    "Add a column to frame whose values are\n",
    "the square root of the b column\n",
    "'''\n",
    "describe(\n",
    "    #your code here\n",
    ")\n",
    "print(hrule(20))\n",
    "\n",
    "'''\n",
    "Comput min, mean, std, max\n",
    "along axis 1 as a new data frame\n",
    "Hint: define a new function\n",
    "'''\n",
    "df_ = #your code here\n",
    "describe(\n",
    "    df_\n",
    ")\n",
    "print(hrule(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f50(x):\n",
    "    return np.where(x>50, 0, x)\n",
    "frame_= frame.apply(f50)\n",
    "frame_['new'] = frame_.b.map({0:'B'}, na_action='ignore')\n",
    "\n",
    "describe(frame_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and ranking\n",
    "\n",
    "Sorting a data set by some criterion is another important built-in operation. To sort\n",
    "lexicographically by row or column index, use the sort_index method, which returns\n",
    "a new, sorted object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series(range(4), index=['d', 'a', 'b', 'c'])\n",
    "frame = DataFrame(np.arange(8).reshape((2, 4)), \n",
    "                  index=['three', 'one'], columns=list(\"dabc\"))\n",
    "\n",
    "print(Hrule(20, \"Sorting Series\"))\n",
    "sorted_obj = obj.sort_index() # sort_index returns a copy\n",
    "describe(sorted_obj) # lex-sorting\n",
    "\n",
    "'''\n",
    "With DatFrames we can choose the\n",
    "axis upon wich performing the sorting operation\n",
    "'''\n",
    "print(Hrule(20, \"Sorting Data-Frames\"))\n",
    "sorted_frame_ax0 = frame.sort_index() # default axis=0\n",
    "describe(sorted_frame_ax0)\n",
    "print(hrule(50))\n",
    "\n",
    "sorted_frame_ax1 = frame.sort_index(axis=1, ascending=False)\n",
    "describe(sorted_frame_ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort a Series by its *values*, use the *sort_vales* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series([4,3,2,1,np.NaN], index=list(\"cdabs\"))\n",
    "objA = obj.sort_values(ascending=True) # it always returns a copy\n",
    "objD = obj.sort_values()  # by default ascending=False\n",
    "describe(objA) \n",
    "print(hrule(20))\n",
    "describe(objD)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: NaNs are always push to the end of the Series, regardless of the order.\n",
    "\n",
    "----\n",
    "### Summarizing and Computing Descriptive Statistics\n",
    "pandas objects are equipped with a set of common mathematical and statistical meth‐\n",
    "ods. Most of these fall into the category of reductions or summary statistics, methods\n",
    "that extract a single value (like the sum or mean) from a Series or a Series of values\n",
    "from the rows or columns of a DataFrame. \n",
    "\n",
    "The main additional fueature provided by pandas\n",
    "methods if compared with the similar methods\n",
    "found on NumPy arrays, is the handling for missing data.\n",
    "\n",
    "\n",
    "Beside all the classic methods for computing statical information\n",
    "along each axis of a datafame, there are two very useful methods that\n",
    "allow us to understand the data we are given with very quickly.\n",
    "\n",
    "These methods are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5],\n",
    "                   [np.nan, np.nan], [0.75, -1.3]],\n",
    "                    index=['a', 'b', 'c', 'd'],\n",
    "                    columns=['one', 'two'])\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The describe method provide summary statistics all at once, while the *info* method\n",
    "provide information about the type of data stored in the dataframe and about\n",
    "the presence of missing-vale with respect any column of the dataframe.\n",
    "\n",
    "The following table describes all the major statistical methods for \n",
    "pandas DataFrame and Serires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(os.path.join(IMG_PATH, 'statistics.png'), width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Values, Value Counts, and Membership\n",
    "Another class of related methods extracts information about the values contained in a\n",
    "one-dimensional Series. To illustrate these, consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])\n",
    "\n",
    "uniqe_values = obj.unique() # return all distinct values as a ndarray\n",
    "\n",
    "describe(uniqe_values)\n",
    "print(hrule(30))\n",
    "\n",
    "# return a Series containing counts of unique values\n",
    "value_count = obj.value_counts(sort=True) \n",
    "describe(value_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique values are not necessarily returned in sorted order, but could be sorted\n",
    "after the fact if needed ( uniques.sort() )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqe_values.sort()\n",
    "describe(uniqe_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table describes several methods related\n",
    "to sets operation with Series\n",
    "\n",
    "|Method|Description|\n",
    "|--|--|\n",
    "| isin |Compute boolean array indicating whether each Series value is contained in the passed sequence of values |\n",
    "| match | Compute integer indices for each value in an array into another array of distinct values; helpful for data alignment and join-type operation |\n",
    "| unique | Compute array of unique values in a Series, returned in the order observed |\n",
    "| value_counts | Return a Series containing unique values as its index and frequencies as its values, ordered count in descending order |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Storage, and File Formats\n",
    "\n",
    "pandas features a number of functions for reading tabular data as a DataFrame\n",
    "object. Table 6-1 summarizes some of them, though read_csv and read_table are\n",
    "likely the ones you’ll use the most.\n",
    "\n",
    "| Function | Description |\n",
    "|--|--|\n",
    "|read_csv| Load delimited data from a file, URL, or file-like object; use comma as default delimiter |\n",
    "|read_table| Load delimited data from a file, URL, or file-like object; use tab ( '\\t' ) as default delimiter |\n",
    "|read_fwf | Read data in fixed-width column format (i.e., no delimiters) |\n",
    "|read_clipboard | Version of read_table that reads data from the clipboard; useful for converting tables from web pages|\n",
    "|read_excel|Read tabular data from an Excel XLS or XLSX file |\n",
    "|read_hdf | Read HDF5 files written by pandas |\n",
    "|read_html | Read all tables found in the given HTML document |\n",
    "|read_json | Read data from a JSON (JavaScript Object Notation) string representation |\n",
    "|read_msgpack | Read pandas data encoded using the MessagePack binary format |\n",
    "|read_pickle | Read an arbitrary object stored in Python pickle format |\n",
    "|read_sas | Read a SAS dataset stored in one of the SAS system’s custom storage formats |\n",
    "|read_sql | Read the results of a SQL query (using SQLAlchemy) as a pandas DataFrame |\n",
    "|read_stata | Read a dataset from Stata file format |\n",
    "|read_father |Read the Feather binary file format |\n",
    "\n",
    "All of these funciton are meant to convert text data into a DataFrame. The optional arguments for these methods can be categorized\n",
    "as:\n",
    "\n",
    "> Indexing : Can treat one or more columns as the returned DataFrame, and whether to get\n",
    "    column names from the file, the user, or not at all.\n",
    "    \n",
    "> Type inference and data conversion : This includes the user-defined value conversions and custom list of missing value\n",
    "    markers    \n",
    "    \n",
    "> Datetime parsing : Includes combining capability, including combining date and time information spread over multiple columns into a single column in the result.\n",
    "\n",
    "> Iterating : Support for iterating over chunks of very large files\n",
    "\n",
    "> Unclead data issues : Skipping rows or a footer, comments, or other minor things like numeric data with thousands separated by commas.\n",
    "\n",
    "---\n",
    "\n",
    "Let's focus on the (likely) the only function you will use. \n",
    "You can read it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('ex1.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default pandas interprets the first row as the one containing the name\n",
    "of each column. Also, by default pandas uses ',' as the character separating each column. \n",
    "\n",
    "We can prevent this behavior in different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(Data('ex1.csv'), header=None) # the first row is considered as part of the data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list contains the names of the column\n",
    "# the first row is still considered as part of the data   \n",
    "df2 = pd.read_csv(Data('ex1.csv'), \n",
    "                  names=[\"a1\", \"a2\", \"a3\", \"a4\", \"a5\"])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a csv file with column separated by a tab, such as the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/ex2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either use *pd.read_table* or *pd.read_csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(Data('ex2.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('ex2.csv'), sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted the message column to be the index of the returned DataFrame.\n",
    "You can either indicate you want the column at index 4 or named 'message' using\n",
    "the index_col argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['a', 'b', 'c', 'd', 'message']\n",
    "df = pd.read_csv(Data('ex2.csv'), names=names, sep=\"\\t\", index_col='message')\n",
    "print(df.head())\n",
    "print(hrule(50))\n",
    "# of course we can use\n",
    "print(df.loc['hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: There is something wrong with the previous instruction.\n",
    "The first row is wrongly interpreted as part of the data.\n",
    "You can prevent this behavior with the *skiprows* argument. \n",
    "\n",
    "*skiprows* accept a list of integer, each one represent the position\n",
    "of the row you want to skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('ex2.csv'), names=names, sep=\"\\t\", index_col='message', skiprows=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas' read_csv function has almost 50 different arguments. We do not have time for that! \n",
    "\n",
    "The following table summerizes some frequently used options related to both *read_csv* and *read_table*.\n",
    "\n",
    "|Argument | Description|\n",
    "|--|--|\n",
    "| path | String indicating filesystem location, URL, or file-like object | \n",
    "|sep, delimiter | Character sequence or regular expression to use to split fields in each row |\n",
    "| header | Row number to use as column names; defaults to 0 (first row), but should be None if there is no header row |\n",
    "|index_col | Column numbers or names to use as the row index in the result; can be a single name/number or a list of them for a hierarchical index |\n",
    "| names | List of column names for result, combine with header=None |\n",
    "|skiprows | Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip. |\n",
    "|na_values | Sequence of values to replace with NA. |\n",
    "|comment | Character(s) to split comments off the end of lines. |\n",
    "|parse_dates | Attempt to parse data to datetime ; False by default. If True , will attempt to parse all columns. |\n",
    "|keep_date_col | If joining columns to parse date, keep the joined columns; False by default. |\n",
    "|converters |  Dict containing column number of name mapping to functions (e.g., {'foo': f} would apply the function f to all values in the 'foo' column). |\n",
    "|dayfirst | When parsing potentially ambiguous dates, treat as international format (e.g., 7/6/2012 -> June 7, 2012); False by default. |\n",
    "|date_parser |  Function to use to parse dates. |\n",
    "|nrows | Number of rows to read from beginning of file. |\n",
    "|iterator | Return a TextParser object for reading file piecemeal. |\n",
    "|chunksize | For iteration, size of file chunks. | \n",
    "|skip_footer | Number of lines to ignore at end of file. |\n",
    "|verbose | Print various parser output information, like the number of missing values placed in non-numeric columns.|\n",
    "|encoding | Text encoding for Unicode (e.g., 'utf-8' for UTF-8 encoded text).|\n",
    "|squeeze | If the parsed data only contains one column, return a Series |\n",
    "|thousands | Separator for thousands (e.g., ',' or '.' ). |\n",
    "\n",
    "\n",
    "### Writing Data\n",
    "It is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(Data('ex2.csv'))\n",
    "df.to_csv(Data('ex2_.csv'))\n",
    "!cat data/ex2_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the first column! By default pandas saves the index of the dataframe along with the data.\n",
    "\n",
    "You can prevent this behavior as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "df.to_csv(Data('ex2_.csv'), index=False)\n",
    "!cat data/ex2_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify a different separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(Data('ex2_.csv'), index=False, sep=\";\")\n",
    "!cat data/ex2_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to skip the column names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(Data('ex2_.csv'), index=False, sep=\";\", header=False)\n",
    "!cat data/ex2_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading JSON\n",
    "\n",
    "JSON (short for JavaScript Object Notation) has become one of the standard formats\n",
    "for sending data by HTTP request between web browsers and other applications. It is\n",
    "a much more free-form data format than a tabular text form like CSV. Here is an\n",
    "example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "[\n",
    "{\"a\": 1, \"b\": 2, \"c\": 3},\n",
    "{\"a\": 4, \"b\": 5, \"c\": 6},\n",
    "{\"a\": 7, \"b\": 8, \"c\": 9}\n",
    "]\n",
    "\"\"\"\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a JSON object is very close to a built-in python array. \n",
    "\n",
    "In fact you can easily convert the two objects as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = json.loads(obj)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And cnvert it back as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asjson = json.dumps(result)\n",
    "#store it into a file\n",
    "with open(Data('ex.json'), 'w') as f:\n",
    "    f.write(asjson)\n",
    "!cat data/ex.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas.read_json can automatically convert JSON datasets in specific arrangements into a Series or DataFrame. \n",
    "\n",
    "The default options for pandas.read_json assume that each object in the JSON array\n",
    "is a row in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(Data('ex.json'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to export data from pandas to JSON, one way is to use the to_json meth‐\n",
    "ods on Series and DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no path is specified the method return \n",
    "# the dataframe as a string\n",
    "record_orientation = df.to_json(orient='records')\n",
    "index_orientation = df.to_json(orient='index')\n",
    "print(\"record:{}\\nindex:{}\".format(record_orientation, index_orientation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Web APIs\n",
    "\n",
    "Many websites have public APIs providing data feeds via JSON or some other format.\n",
    "There are a number of ways to access these APIs from Python; one easy-to-use\n",
    "method is the the **request** package.\n",
    "\n",
    "Imagine you have to download a dataset from the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_data(url):\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return pd.read_csv(StringIO(resp.text))\n",
    "    return None\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\n",
    "df = download_data(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the easiest way it to pass the url directly to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(url).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "\n",
    "During the course of doing data analysis and modeling, a significant amount of time\n",
    "is spent on data preparation: loading, cleaning, transforming, and rearranging. Such\n",
    "tasks are often reported to take up 80% or more of an analyst’s time. Sometimes the\n",
    "way that data is stored in files or databases is not in the right format for a particular\n",
    "task.\n",
    "\n",
    "### Handling Missing Data\n",
    "\n",
    "One of the goals\n",
    "of pandas is to make working with missing data as painless as possible. For example,\n",
    "all of the descriptive statistics on pandas objects exclude missing data by default.\n",
    "\n",
    "When a particular value is missing pandas places a **NaN** (i.e., the NumPy not-a-number) symbol which stands\n",
    "for \"Not Available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import NaN as nan\n",
    "obj = Series(['luka', 'doncic', nan, 'ROY', None]) \n",
    "\n",
    "describe(obj)\n",
    "\n",
    "print(Hrule(20, \"Select nan values\"))\n",
    "nulls = obj.isnull() # return a mask pointing to every missing value\n",
    "describe(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the built-in *None* is treated as a *None*.\n",
    "\n",
    "#### Filtering Missing Data\n",
    "\n",
    "While filtering missing data we can either use the mask returned by the *isnull* \n",
    "(there is also a method *notnull*) or use the *dropna* function.\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null = obj.dropna() # it returns a copy\n",
    "describe(not_null) # the row at index 2 has been removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: write an equivalent method for removing null values from obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(\n",
    "    #your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regard the DataFrame, when using the dropna method you must specify the axis along which you want to perform\n",
    "the action. You can either remove rows or columns (entirely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by default pandas remove an entire record (a row or column) as it finds even just a single missing value.\n",
    "\n",
    "As a consequence this single missing value can cause a massive loss of information. \n",
    "You override this behavior by specifying a different value for the *how* argument of dropna (by default it is 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with *how='all'* you are telling to pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than removing entire recors we can decide to fill the missing values.\n",
    "\n",
    "You can do it in several ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "Duplicate rows may be found in a DataFrame for any number of reasons. Here is an\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'], 'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "print(df.head())\n",
    "\n",
    "print(Hrule(20, \"Get duplicated values\"))\n",
    "duplicated = df.duplicated() # return a mask \n",
    "print(duplicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to remove all the duplicated values you can use the *drop_duplicates* method.\n",
    "drop_duplicates returns a new DataFrame whose value correspond to the \n",
    "value of the original DataFrame where the duplicated Series is False :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates() # the six entry is gone!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: By The membership is evaluated considering the entire row.\n",
    "Alternatively, you can specify any subset of them to detect duplicates.\n",
    "Suppose we had an additional column\n",
    "of values and wanted to filter duplicates only based on the 'k1' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(['k1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The element that pandas keeps is always the first occurence of a given value. \n",
    "You can override this behavior as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(['k1'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Data Using a Function or Mapping\n",
    "\n",
    "For many datasets, you may wish to perform some transformation based on the val‐\n",
    "ues in an array, Series, or column in a DataFrame.\n",
    "\n",
    "Consider the following hypotheti‐\n",
    "cal data collected about various kinds of meat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'],'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\n",
    "meat_to_animal = {\n",
    "    'bacon': 'pig',\n",
    "    'pulled pork': 'pig',\n",
    "    'pastrami': 'cow',\n",
    "    'corned beef': 'cow',\n",
    "    'honey ham': 'pig',\n",
    "    'nova lox': 'salmon'\n",
    "}\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to add a column indicating the type of animal that each food\n",
    "came from. Let’s write down a mapping of each distinct meat type to the kind of\n",
    "animal\n",
    "\n",
    "The map method on a Series accepts a function or dict-like object containing a map‐\n",
    "ping, but here we have a small problem in that some of the meats are capitalized and\n",
    "others are not. Thus, we need to convert each value to lowercase using the *str.lower*\n",
    "Series method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['animal'] = data.food.str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass the dict *meat_to_animal* to the map function over the desired column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['animal'] = data.animal.map(meat_to_animal)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create the animal column (call it ``animal_``) in one line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the *map* method can be used for replacing values in a DataFrame there is \n",
    "a more suitable way of doing it. In fact, you can use the replace function.\n",
    "\n",
    "You can pass either a pair of scalar values, a pair of array-like values or a dict.\n",
    "\n",
    "The replace function, unless you pass inplace=True, always returns a copy of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series([1., -999., 2., -999., -1000., 3.])\n",
    "\n",
    "obj.replace(-999, nan)\n",
    "describe(obj)\n",
    "print(hrule(50))\n",
    "\n",
    "# the replacement is pairwise\n",
    "obj.replace([-999, 2], [-nan, -2], inplace=True)\n",
    "describe(obj)\n",
    "\n",
    "#the arguments don not need to have the same size\n",
    "obj.replace([-nan, -2],-10, inplace=True)\n",
    "describe(obj)\n",
    "                 \n",
    "# you can also pass a dict\n",
    "obj.replace({-10:-99999}, inplace=True)\n",
    "describe(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(np.random.randint(0,50,12).reshape(4, 3), columns=list('abc'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "df = df.astype(np.float) # convert the dataset to floating point\n",
    "df.values[[0,1,2], [0,2,0]] = nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a new column 'd' where\n",
    "there is 'Missing' if \n",
    "the corresponing row contains\n",
    "a Nan in the column 'a', otherwise\n",
    "'d' will contain the value 'not missing'\n",
    "\n",
    "Hint: remember np.where?\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and Binning\n",
    "\n",
    "Continuous data is often discretized or otherwise separated into “bins” for analysis.\n",
    "\n",
    "Suppose you have data about a group of people in a study, and you want to group\n",
    "them into discrete age buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "bins = [18, 25, 35, 60, 100]\n",
    "cats = pd.cut(ages, bins)\n",
    "describe(cats)\n",
    "print(hrule(50))\n",
    "df = pd.DataFrame(np.column_stack([ages, cats]), columns=['Age', 'Bin'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object pandas returns is a special Categorical object. The output you see\n",
    "describes the bins computed by pandas.cut . You can treat it like an array of strings\n",
    "indicating the bin name; internally it contains a categories array specifying the dis‐\n",
    "tinct category names along with a labeling for the ages data in the codes attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"codes:{}\\ncategories:{}\".format(cats.codes, cats.categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the number of element inside each bin as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cats.value_counts())\n",
    "print(hrule(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *cut* operation is of course highly customizable.\n",
    ">If you specify *right=False* the intervals become closed to the left\n",
    "\n",
    ">With *labels=[...list of group names...]* you can specify the name to assign to each group\n",
    "\n",
    ">With the argument *precision* you can set the number of decimal value to take into account while ''binning'' the Series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bin'] = pd.cut(ages,4,right=False, precision=0)\n",
    "df['Label'] = pd.cut(ages,4, labels=['kid', 'teen', 'young', 'old'], right=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing a list of bins you can just specify the number of bins you whish to devide your data.\n",
    "By default *cut* split the data accordingly to an equal-frequency strategy.\n",
    "\n",
    "However if you wish to spiit the data in quantiles you can use the *qcut* function.\n",
    "\n",
    "\n",
    "#### Permutation and Random Sampling\n",
    "\n",
    "Permuting (randomly reordering) a Series or the rows in a DataFrame is easy to do\n",
    "using the numpy.random.permutation function.\n",
    "\n",
    "Calling permutation with the length\n",
    "of the axis you want to permute produces an array of integers indicating the new\n",
    "ordering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it returns 10 number whitin [0,10) in random order\n",
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use the permuation method to extract a subset from a dataframe (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler way to reordering from the dataframe is via the functions *take*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.take(np.random.permutation(df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Extracting a sample from the datafram is also very simple. \n",
    "You just need to use the *sample* function. \n",
    "\n",
    "To generate a sample with replacement (to allow repeat choices), pass replace=True (by default is False)\n",
    "to sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n=3, replace=True)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Indicator/Dummy Variables\n",
    "\n",
    "Another type of transformation for statistical modeling or machine learning applica‐\n",
    "tions is converting a categorical variable into a “dummy” or “indicator” matrix. \n",
    "\n",
    "If a  column in a DataFrame has k distinct values, you would derive a matrix or Data‐\n",
    "Frame with k columns containing all 1s and 0s. pandas has a get_dummies function\n",
    "for doing this, though devising one yourself is not difficult. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.get_dummies(df['key'], prefix='key') \n",
    "df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we have just transforme the key-column into a one-hot encoding vector!\n",
    "\n",
    "Now, you have a one-hot encoded dataframe. You may wonder how this can be included in the original\n",
    "dataframe. Be patient we will see how to do it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized String Functions in pandas\n",
    "\n",
    "Cleaning up a messy dataset for analysis often requires a lot of string munging and\n",
    "regularization. To complicate matters, a column containing strings will sometimes\n",
    "have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',\n",
    "    'Rob': 'rob@gmail.com', 'Wes': np.nan}\n",
    "data = Series(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any string and regular expression methods can be applied (passing a\n",
    "lambda or other function) to each value using the *map* function.\n",
    "\n",
    "However, you will struggle with NA\n",
    "(null) values. \n",
    "\n",
    "To cope with this, Series has array-oriented methods for string opera‐\n",
    "tions that skip NA values. \n",
    "\n",
    "These are accessed through Series’s **str** attribute. In this way you can directly access\n",
    "to the string underlying the each element stored in the Series object without considering\n",
    "the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# after accessing the str attribute you can\n",
    "# perform any string-manipulation function (including regex)\n",
    "print(data.str.contains('gmail')) \n",
    "print(hrule(20))\n",
    "\n",
    "#extract different information from the string\n",
    "#using regular expressions\n",
    "print(data.str.findall('([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})', flags=re.IGNORECASE))\n",
    "print(hrule(20))\n",
    "\n",
    "#check wheter or not a string repsect a certain pattern\n",
    "print(data.str.match('([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})', flags=re.IGNORECASE))\n",
    "print(hrule(20))\n",
    "#create a dataframe. Each column correspond to a matched group\n",
    "describe(data.str.extract('([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})', flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a data frame starting from the data Series with two column:\n",
    "the first column contains the username of a user, the second one contains the domain of the email-address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(series):\n",
    "    first, second = series.str.split('@').str\n",
    "    return pd.DataFrame({'username':first, 'domain': second}, index=series.index)\n",
    "describe(extract(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table with almost every method for handling string objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(Img('tab.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling: Join, Combine, and Reshape\n",
    "### Hierarchical Indexing\n",
    "\n",
    "Hierarchical indexing is an important feature of pandas that enables you to have multiple (two or more) index levels on an axis. Somewhat abstractly, it provides a way for\n",
    "you to work with higher dimensional data in a lower dimensional form. Let’s start\n",
    "with a simple example; create a Series with a list of lists (or arrays) as the index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(np.random.randn(9), index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'], [1, 2, 3, 1, 3, 1, 2, 2, 3]])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you’re seeing is a prettified view of a Series with a MultiIndex as its index. The\n",
    "“gaps” in the index display mean “use the label directly above”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a hierarchically indexed object, so-called partial indexing is possible, enabling\n",
    "you to concisely select subsets of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['a'])\n",
    "print(hrule(20))\n",
    "\n",
    "print(data.loc[['b', 'd']])\n",
    "print(hrule(20))\n",
    "\n",
    "#inner level selection\n",
    "print(data.loc['a':'b', 1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical indexing plays an important role in reshaping data and group-based\n",
    "operations like forming a pivot table. For example, you could rearrange the data into\n",
    "a DataFrame using its unstack method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.unstack()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first-level index becomes the axis-0 index while the inner-index becomes the axis-1 index (the columns).\n",
    "\n",
    "You can also reverse the operation as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df.stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "With a DataFrame, either axis can have a hierarchical index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.arange(12).reshape((4, 3)), \n",
    "                     index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n",
    "                     columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])\n",
    "print(frame)\n",
    "print(hrule(30))\n",
    "print(frame.loc['a',1])\n",
    "print(hrule(30))\n",
    "print(frame.loc[('b',1),'Ohio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reordering and Sorting Levels\n",
    "At times you will need to rearrange the order of the levels on an axis or sort the data\n",
    "by the values in one specific level. The swaplevel takes two level numbers or names\n",
    "and returns a new object with the levels interchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate a label with each index\n",
    "frame.index.names = ('key1', 'key2')\n",
    "frame.columns.names = ['state', 'color']\n",
    "print(frame)\n",
    "print(Hrule(20, \"swap indexes\"))\n",
    "swapped_frame = frame.swaplevel('key1', 'key2')\n",
    "print(swapped_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When swapping, pandas does not automatically sort the values of the data-frame\n",
    "according to the new configuration.\n",
    "\n",
    "Therefore, we need to use the *sort_index*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(swapped_frame.sort_index(level=0))\n",
    "print(Hrule(20, \"See the difference?\"))\n",
    "print(swapped_frame.sort_index(level=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each level corresponds to a particular index.\n",
    "\n",
    "In this case (after the swapping) key2 corresponds to level=0 while\n",
    "key1 corresponds to level=1.\n",
    "\n",
    "Actually, you do not need to name each index.\n",
    "You can swap them by specifying their level.\n",
    "```\n",
    " df.swaplevel(0,1)\n",
    "```\n",
    "**Exercise**: Make *swapped_frame* as the original fame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many descriptive and summary statistics on DataFrame and Series have a level\n",
    "option in which you can specify the level you want to aggregate by on a particular\n",
    "axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame)\n",
    "print(hrule(30))\n",
    "\n",
    "print(frame.sum(level=1))\n",
    "print(hrule(30))\n",
    "\n",
    "print(frame.mean(level='color', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing with a DataFrame’s columns\n",
    "You ma  want to use one or more columns from a DataFrame as the row\n",
    "index;\n",
    "\n",
    "alternatively, you may wish to move the row index into the DataFrame’s col‐\n",
    "umns.\n",
    "\n",
    "Here’s an example DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({'a': range(7), 'b': range(7, 0, -1),\n",
    "    'c': ['one', 'one', 'one', 'two', 'two',\n",
    "    'two', 'two'],\n",
    "    'd': [0, 1, 2, 0, 1, 2, 3]})\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ = frame.set_index(['a','b'])\n",
    "print(frame_)\n",
    "print(hrule(20))\n",
    "frame_ = frame.set_index(['c', 'd']).sort_index(level=[0,1], ascending=[True, False])\n",
    "print(frame_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the columns selected as index are removed from the DataFrame\n",
    "However you can leave them:\n",
    "```\n",
    "frame.set_index(['c', 'd'], drop=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining and Merging Datasets\n",
    "Data contained in pandas objects can be combined together in a number of ways:\n",
    "\n",
    "1. pandas.merge connects rows in DataFrames based on one or more keys. This will be familiar to users of SQL or other relational databases, as it implements database join operations.\n",
    "2. pandas.concat concatenates or “stacks” together objects along an axis.\n",
    "3. The combine_first instance method enables splicing together overlapping data to fill in missing values in one object with values from another.\n",
    "\n",
    "---\n",
    "\n",
    "Merge or join operations combine datasets by linking rows using one or more keys.\n",
    "These operations are central to relational databases (e.g., SQL-based). The merge\n",
    "function in pandas is the main entry point for using these algorithms on your data.\n",
    "Let’s start with a simple exampl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a'],\n",
    "    'data1': range(5)})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'key': ['a', 'b', 'd'], 'data2': range(3)})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a **many-to-one** join;\n",
    "\n",
    "the data in df1 has multiple rows labeled as a\n",
    "and b, \n",
    "whereas df2 has only one row for each value in the key column. Calling merge\n",
    "with these objects we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1,df2, on='key') # the key column must be present on every data-frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for som reason the columns are named differently you can specify them separately as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "df1 = df1.rename(columns={'key': 'lkey'})\n",
    "df2 = df2.rename(columns={'key': 'rkey'})\n",
    "\n",
    "pd.merge(df1, df2, left_on='lkey', right_on='rkey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that by default pandas makes an inner join, therefore\n",
    "the joint dataframe includes only the rows involved in the join operation.\n",
    "\n",
    "You can override this behavior with the *how* argument. It accepts\n",
    "a strategy according to which the join operation needs to be performed.\n",
    "\n",
    "There are 4 possible choice: \n",
    "1. inner \n",
    "2. outer \n",
    "3. left\n",
    "4. right\n",
    "---\n",
    "**Many-to-many** merges have well-defined, though not necessarily intuitive, behavior.\n",
    "Here’s an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})\n",
    "df2 = pd.DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],'data2': range(5)})\n",
    "print(df1)\n",
    "print(hrule(30))\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many-to-many joins form the Cartesian product of the rows.\n",
    "\n",
    "Since there were three\n",
    "'b' rows in the left DataFrame and two in the right one, there are six 'b' rows in the\n",
    "result. \n",
    "\n",
    "The join method only affects the distinct key values appearing in the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on='key', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you may want to perform the merging operation upon the actual keys of the data frame. \n",
    "\n",
    "In such scenario you must pass to the function merge the following arguments: \n",
    "```\n",
    "    pd.merge(..., left_index=True, right_index=True)\n",
    "```\n",
    "By default, i.e., with how='ineer', you will obtain the intersection of the two frames.\n",
    "\n",
    "**Exercise**: compute the union of df1 and df2 wrt to their indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame(dict(key=list('abaabc'), value=range(6)))\n",
    "df2 = DataFrame({'key': list('abbcdea'), 'value':range(7)})\n",
    "df1, df2 = df1.set_index('key'), df2.set_index('key')\n",
    "\n",
    "print(df1)\n",
    "print(hrule(30))\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame has a convenient join instance for merging by index. \n",
    "\n",
    "It can also be used\n",
    "to combine together many DataFrame objects having the same or similar indexes **but\n",
    "non-overlapping columns**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]],index=['a', 'c', 'e'],columns=['Ohio', 'Nevada'])\n",
    "right = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])\n",
    "\n",
    "print(pd.merge(left, right, left_index=True, right_index=True, how='outer'))\n",
    "print(Hrule(20, \"Is equivalent to\"))\n",
    "print(left.join(right, how='outer'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table containing all the possible merge arguments\n",
    "\n",
    "|Argument|Description|\n",
    "|--|--|\n",
    "|left|DataFrame to be merged on the left side.|\n",
    "|right|DataFrame to be merged on the right side.|\n",
    "|how|One of 'inner' , 'outer' , 'left' , or 'right' ; defaults to 'inner'|\n",
    "|on|Column names to join on. Must be found in both DataFrame objects. If not specified and no other join keys |\n",
    "|left_on | Columns in left DataFrame to use as join keys.|\n",
    "|right_on | Analogous to left_on for left DataFrame.|\n",
    "|left_index | Use row index in left as its join key (or keys, if a MultiIndex).|\n",
    "|right_index | Analogous to left_index. |\n",
    "| sort | Sort merged data lexicographically by join keys; True by default (disable to get better performance in some cases on large datasets)|\n",
    "| suffixes | Tuple of string values to append to column names in case of overlap; defaults to ('_x', '_y') (e.g., if 'data' in both DataFrame objects, would appear as 'data_x' and 'data_y' in result).|\n",
    "| copy | If False , avoid copying data into resulting data structure in some exceptional cases; by default always copies.|\n",
    "| indicator | Adds a special column _merge that indicates the source of each row; values will be 'left_only' , 'right_only' , or 'both' based on the origin of the joined data in each row.|\n",
    "\n",
    "---\n",
    "\n",
    "### Concatenating Along an Axis\n",
    "In the context of pandas objects such as Series and DataFrame, having labeled axes\n",
    "enable you to further generalize array concatenation. In particular, you have a num‐\n",
    "ber of additional things to think about:\n",
    "1. If the objects are indexed differently on the other axes, should we combine the distinct elements in these axes or use only the shared values (the intersectio)?\n",
    "2. Do the concatenated chunks of data need to be identifiable in the resulting object?\n",
    "3. Does the “concatenation axis” contain data that needs to be preserved? In many cases, the default integer labels in a DataFrame are best discarded during concatenation\n",
    "\n",
    "The concat function in pandas provides a consistent way to address each of these\n",
    "concerns. \n",
    "\n",
    "Suppose we have\n",
    "three Series with no index overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = Series([0, 1], index=['a', 'b'])\n",
    "s2 = Series([2, 3, 4], index=['c', 'd', 'e'])\n",
    "s3 = Series([5,6], index=['f', 'g'])\n",
    "\n",
    "print(pd.concat([s1, s2, s3])) # by default concat works on axis 0\n",
    "print(hrule(20))\n",
    "print(pd.concat([s1,s2,s3], axis=1, sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Non-overlapping indexes are filled with NaN (it is like an outer join).\n",
    "\n",
    "If you want to retain only the common values you can do it with the *join* argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = Series([0,2,4], index=['a', 'b', 'e'])\n",
    "pd.concat([s1, s4], axis=1,  sort=False, join='inner') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you need to identify the concatenated piece in the resultyou can create a hierarchical index as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([s1, s2, s3], keys=['s1', 's2', 's3'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same operation on axis=1 produce a DataFrame where each column is named after the element in keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([s1, s2, s3], keys=['s1', 's2', 's3'], axis=1, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same logic applies on DataFrame as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame(np.arange(6).reshape(3,2),index=list('abc'), columns=['one', 'two'])\n",
    "df2 = DataFrame(np.arange(4).reshape(2,2),index=list('ac'), columns=['three', 'four'])\n",
    "print(df1)\n",
    "print(hrule(20))\n",
    "print(df2)\n",
    "print(hrule(20))\n",
    "print(pd.concat([df1,df2], axis=1, keys=['level1', 'level2'], names=['upper', 'lower'], sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftem, you just need to concatenate multiple dataset without concerning about the the index, because it\n",
    "could provide irrelevant information.\n",
    "\n",
    "In this case you must specify the argument *ignore_index=True*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame(np.arange(10).reshape(2,5), columns=list('abcde'))\n",
    "df2 = DataFrame(np.arange(10).reshape(2,5), columns=list('abcdf'))\n",
    "\n",
    "pd.concat([df1, df2], axis=0, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and Pivoting\n",
    "\n",
    "#### With Hierarchical Indexing\n",
    "There are tow main instructions:\n",
    "1. stack: pivots from the columns to the rows\n",
    "2. unstack : pivots from the rows to the columns (inverse of stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(np.arange(6).reshape(2,3), index=pd.Index(['Ohio', 'Colorado'], name='state'), columns=pd.Index(['one', 'two', 'three'], name='number'))\n",
    "print(data)\n",
    "print(Hrule(20, \"Stacking\"))\n",
    "\n",
    "# produce a dataframe with a hierarchical index (state, number)\n",
    "stacked = data.stack()\n",
    "print(stacked)\n",
    "# accessing element\n",
    "print(\"Ohio->one:{}\".format(stacked.loc['Ohio', 'one']))\n",
    "print(Hrule(20, \"Unstacking\"))\n",
    "\n",
    "unstacked = stacked.unstack() # return in the original form\n",
    "print(unstacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the innermost level is stacked (or unstacked). \n",
    "\n",
    "However, you can modify this behavior by sepcifying the level of the index explictly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacked.unstack(0)) # the outer index becomes the column index\n",
    "print(Hrule(20, \"See the difference?\"))\n",
    "print(stacked.unstack(1)) # the inner index becomes the column index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting\n",
    "Return reshaped DataFrame organized by given index / column values.\n",
    "\n",
    "Reshape data (produce a “pivot” table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('nba.csv'), usecols=['PLAYER', 'POSITION', 'SALARY_MILLIONS', 'TEAM'], nrows=100)\n",
    "df.drop_duplicates(subset='PLAYER', keep='first', inplace=True)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivoting\n",
    "pivoted = df.pivot(index='POSITION',  columns=\"PLAYER\", values=\"SALARY_MILLIONS\")\n",
    "pivoted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melting\n",
    "An inverse operation to pivot for DataFrames is pandas.melt . Rather than trans‐\n",
    "forming one column into many in a new DataFrame, it merges multiple columns into\n",
    "one, producing a DataFrame that is longer than the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted = pd.melt(df, ['TEAM'])\n",
    "melted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column becomes a value under the 'variable' column of the melted DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df columns: {} vs melted variable values: {}\".format(df.columns.values,  melted['variable'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated way of using the melt operation would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider only the POSITION and PLAYER columns\n",
    "melt = pd.melt(df, id_vars=['TEAM'], value_vars=['POSITION', 'PLAYER'])\n",
    "print(melt.head())\n",
    "print(melt.variable.unique())\n",
    "\n",
    "#actually we can also omit the id_vars argument\n",
    "print(hrule(50))\n",
    "print(pd.melt(df, value_vars=[\"POSITION\", \"PLAYER\"]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation and Group Operations\n",
    "\n",
    "The entire process of data aggregation can be summerized by the following image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(Img('sac.png'), width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image represents a three-stages paradigm called *split-apply-combine*.\n",
    "\n",
    "> The first stage of the process is concerned with the splitting of the data into several groups based on some criteria\n",
    "\n",
    "> The second stage is concerned with the application of a function upon the grouped data\n",
    "\n",
    "> The third and final stage asddress the problem of collecting and presenting the final result, i.e, the result obtained by the application of the function performed in the second stage.\n",
    "\n",
    "The first part of the process is performed by the group_by method.\n",
    "\n",
    "It needs a list of columns names, i.e, the columns upon which the grouping operation has to be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randn(5), 'data2' : np.random.randn(5)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_object = df.groupby(by=['key1', 'key2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the grouped object you can apply any function. \n",
    "\n",
    "For instance, if you want to compute the size each group into which the datast has been splitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_object.size())\n",
    "\n",
    "#of course you can do it in one line of code\n",
    "result = df.groupby('key1').mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Let's focus on the dataframe *result*. What are its index? \n",
    ">**Answer** : you got to write it!\n",
    "\n",
    "---\n",
    "\n",
    "If you want to prevent this behavior, you can call the reset_index method upon the \n",
    "final data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reset_index() \n",
    "# or equivalently\n",
    "# result = df.groupby('key1', as_index=False).mean()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating Over Groups\n",
    "Sometimes, you may need to iterate over the groups generated by the previous stage.\n",
    "\n",
    "The grouped object provide a way of generating a sequence of 2-tuples containg\n",
    "the group name and the group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('key1'):\n",
    "    print(name)\n",
    "    print(group, type(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case of grouping wrt multiple columns\n",
    "for (k1, k2), group in df.groupby(['key1', 'key2']):\n",
    "    print(k1,k2)\n",
    "    print(group, type(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Each group is a simple DataFrame, and of course you can treat it as a regular DataFrame object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Column of Subset of Columns\n",
    "Indexing a GroupBy object created from a DataFrame with a column name or array\n",
    "of column names has the effect of column subsetting for aggregation.\n",
    "\n",
    "Here are several way for selecting along with the group_by operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df.groupby('key1')['data1'].mean())\n",
    "print(Hrule(20, \"Is equivalent to\"))\n",
    "describe(df['data1'].groupby(df['key1']).mean())\n",
    "\n",
    "print(hrule(50))\n",
    "print(hrule(50))\n",
    "\n",
    "describe(df.groupby('key1')[['data2']].mean())\n",
    "print(Hrule(20, \"Is equivalent to\"))\n",
    "describe(df[['data2']].groupby(df['key1']).mean())\n",
    "print(hrule(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The first two objects are Series while the second two objects are DataFrame.\n",
    "\n",
    "The reason underlying this behavior resides in the notation used for selecting the column.\n",
    "\n",
    "In fact, it should be noted that ``df[['col_name]]`` (double square bracket) returns a DataFrame as opposed\n",
    "to ``df[col_name]`` which return a Series object.\n",
    "\n",
    "### Grouping with Dicts and Series\n",
    "Grouping information may exist in a form other than an array. Let’s consider another\n",
    "example DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.DataFrame(np.random.randn(5, 5),\n",
    "    columns=['a', 'b', 'c', 'd', 'e'],\n",
    "    index=['Joe', 'Steve', 'Wes', 'Kendrick', 'Travis'])\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that you have knowledge that columns a-b-c form a group A, and columns d-e form another group B.\n",
    "\n",
    "You want to compute some statistic wrt to groups A,B.\n",
    "\n",
    "One way of doing so is to remap each column in order to embrace the group A and B distinction and then apply the grouping operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'a': 'A', 'b':'B', 'c':'A', 'd':'A', 'e':'A'}\n",
    "\n",
    "# axis=1 means that we are grouping the columns\n",
    "people.groupby(mapping, axis=1).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Functions\n",
    "\n",
    "Using Python functions is a more generic way of defining a group mapping compared\n",
    "with a dict or Series. \n",
    "Any function passed as a group key will be called once per index\n",
    "value, with the return values being used as the group names. \n",
    "\n",
    "More concretely, consider the previous example (people dataframe),\n",
    "with names as index values.\n",
    "Suppose you want to group by the length of the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,g in people.groupby(len):\n",
    "    print(\"name:{}\\ndata:{}\".format(n,g))\n",
    "    print(hrule(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation\n",
    "\n",
    "Aggregations refer to any data transformation that produces scalar values from\n",
    "arrays\n",
    "\n",
    "Many common aggregations, such as those found in the following table,\n",
    "have optimized implementations. However, you are not limited to only this set of\n",
    "methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image(Img('tabagg.png'), width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own aggregation functions, pass any function that aggregates an array to\n",
    "the aggregate or agg method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_to_peak(arr):\n",
    "    return arr.max() - arr.min()\n",
    "print(df)\n",
    "print(hrule(50))\n",
    "print(df.groupby(['key1', 'key2']).agg(peak_to_peak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-Wise and Multiple Function Application\n",
    "\n",
    "Instead of using directly methods like *mean, sum....* we can take advantage of the more general\n",
    "aff function.\n",
    "\n",
    "In fact the ``agg(...)`` function accepts a list of function which are perfomed as a pipeline.\n",
    "\n",
    "Let's consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data('tips.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you want to compute the mean and the standard deviation \n",
    "grouped by day and smoker. \n",
    "\n",
    "You can do it in one line of ocde as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['day', 'smoker']).agg(('mean','std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify different funcion for different columns.\n",
    "\n",
    "You just need to specify a list of 2-tuple as (c,f) where c is the name of a column while\n",
    "f is the a function.\n",
    "\n",
    "> **Note**: c is actually the name of the resulting column not the one in the original dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['day', 'smoker']).agg([('mean_column', 'mean'),('std_column', np.std)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
