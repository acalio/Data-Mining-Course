{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8 - Project Example\n",
    "\n",
    "In this lecture we are going to discuss and analyze the structure of a \n",
    "good-quality analysis for the final project of this course.\n",
    "\n",
    "## Titanic, Learning from the disaster\n",
    "\n",
    "---\n",
    "__Suggestions__:\n",
    "At this stage, you should answer to the following questions (not necessarily all of them):\n",
    "\n",
    "1. Define the objective in business term\n",
    "2. How will your solution be used?\n",
    "3. What are the current solutions/workarounds (if any)?\n",
    "4. How should you frame this problem (supervised/unsupervised, online/offline learning)?\n",
    "5. How should performance be measured?\n",
    "6. Is the performance measure aligned with the business objective?\n",
    "7. What would be the minimum performance needed to reach the business objective?\n",
    "8. What are comparable problems? Can you reuse experience or tools?\n",
    "9. Is human expertise available?\n",
    "10. How would you solve the problem manually?\n",
    "11. List the assumptions made so far and verify them.\n",
    "\n",
    "---\n",
    "\n",
    "The main goal of this project is to solve the following task:\n",
    "    \n",
    "> __Problem Statement__: Given information about people who unfortunately were embarked on the titanic, deciding wether or not\n",
    "  a person will survive shipwreck or not.\n",
    "  \n",
    "The solution provided in this work is developed to be a stand-alone solution, therefore the proposed approach does not have any requirements\n",
    "about compatibility of integration with other models.\n",
    "\n",
    "It appears that the best strategy for tackling the problem is by the means of a classification algorithm, thus we are going solve a supervised \n",
    "learning problem.\n",
    "\n",
    "Since we decided to tackle the problem, the best way for measuring the performance would be computing any classification-related error measures,\n",
    "e.g., accuracy, confusion matrix, recall... (initially you can be vague, you may want to return later for a further description).\n",
    "\n",
    "\n",
    "## Get the Data\n",
    "\n",
    "---\n",
    "__Suggestions__:\n",
    "In this section, you should address the following point (again, you are not required to address every single point):\n",
    "\n",
    "0. Introduce the reader to the data. Provide a first qualitative analysis.\n",
    "1. List the data you need and how much you need.\n",
    "2. Find and document where you can get that data.\n",
    "3. Check how much space it will take.\n",
    "4. Check legal obligations, and get authorization if necessary.\n",
    "5. Get access authorizations.\n",
    "6. Create a workspace (with enough storage space).\n",
    "7. Get the data.\n",
    "8. Convert the data to a format you can easily manipulate (without changing the\n",
    "data itself).\n",
    "9. Ensure sensitive information is deleted or protected (e.g., anonymized).\n",
    "10. Check the size and type of data (time series, sample, geographical, etc.).\n",
    "11. Sample a test set, put it aside, and never look at it (no data snooping!).\n",
    "\n",
    "---\n",
    "The dataset contains information like gender, ticket class, age, etc. related to the titanic passenger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more detailed description about the meaning of each attribute and its values:\n",
    "\n",
    "* PassengerID - A column added by Kaggle to identify each row and make submissions easier\n",
    "* Survived - Whether the passenger survived or not and the value we are predicting (0=No, 1=Yes) __(*)__\n",
    "* Pclass - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd)\n",
    "* Sex - The passenger's sex\n",
    "* Age - The passenger's age in years\n",
    "* SibSp - The number of siblings or spouses the passenger had aboard the Titanic\n",
    "* Parch - The number of parents or children the passenger had aboard the Titanic\n",
    "* Ticket - The passenger's ticket number\n",
    "* Fare - The fare the passenger paid\n",
    "* Cabin - The passenger's cabin number\n",
    "* Embarked - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)\n",
    "\n",
    "---\n",
    "\n",
    "__(*)__ this is thea attribute containing the class we aim to predict\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "---\n",
    "__Suggestions__:\n",
    "1. Create a copy of the data for exploration (sampling it down to a manageable size\n",
    "    if necessary).\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration.\n",
    "3. Study each attribute and its characteristics like:\n",
    "    * Name, Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "    * Percentage of missing values\n",
    "    * Noisiness and type of noise 8tochastic, outliers, rounding errors, etc.)\n",
    "    * Type of distribution, e.g., Gaussian, uniform, logarithmic, etc.\n",
    "4. For supervised learning tasks, identify the target attribute(s).\n",
    "5. Visualize the data.\n",
    "6. Study the correlations between attributes.\n",
    "7. Study how you would solve the problem manually.\n",
    "8. Identify the promising transformations you may want to apply.\n",
    "9. Identify extra data that would be useful (go back to “Get the Data” on page 498).\n",
    "10. Document what you have learned.\n",
    "---\n",
    "\n",
    "Since the training set has a rather small size we can explore the data considering the entire dataset.\n",
    "The dataset dimensions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following instruction we chekc wether or not there are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that some attributes have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.isnull().sum()/df.shape[0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, ``Age`` and ``Cabin`` present missing values (We will deal with this problem later).\n",
    "\n",
    "Let's take a quick look at the numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The main goal of this stage is to understand what variables are important for predicting wether or not \n",
    "a person will survive the shipwreck.\n",
    "\n",
    "Two attributes that may be very promising are the age and the class of the ticket. \n",
    "\n",
    "In fact, in general, in this scenario, children and women are the category with the most priority.\n",
    "In order to verify this assumption we can issue the following instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sex = df.groupby(['Sex', 'Survived'])[['PassengerId']].count()\n",
    "df_sex = df_sex.rename({'PassengerId': 'count'}, axis=1)\n",
    "df_sex['prob'] = df_sex['count'] / df.groupby('Sex').count()['Survived']\n",
    "df_sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, one may expects that passengers with higher ticket class would have more privilege than low class passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pclass = df.groupby(['Pclass', 'Survived'])[['PassengerId']].count()\n",
    "df_pclass = df_pclass.rename({'PassengerId': 'count'}, axis=1)\n",
    "df_pclass['prob'] = df_pclass['count'] / df.groupby('Pclass').count()['Survived']\n",
    "df_pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we conduct the same kind of analysis for the age of the passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Age`` is a real-valued attribute. Therefore it is better to plot the age distribution wrt to \n",
    "the ``Survived`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived = df[df[\"Survived\"] == 1]\n",
    "died = df[df[\"Survived\"] == 0]\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].hist(survived[\"Age\"].dropna(), alpha=0.5, color='red', bins=30)\n",
    "axes[1].hist(died[\"Age\"].dropna() ,alpha=0.5,color='blue',bins=30)\n",
    "axes[0].set_title('Survived')\n",
    "axes[1].set_title('Not Survived')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[1].set_xlabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that only a small fraction of people of age within 0 and 20 does not survive the shipwreck.\n",
    "\n",
    "In order to gather some other insight we plot the same histograms, but this times we distinguish between \n",
    "male and female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived = df[df[\"Survived\"] == 1]\n",
    "died = df[df[\"Survived\"] == 0]\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].hist(survived.loc[survived[\"Sex\"] == \"male\"][\"Age\"].dropna(), label='Male', alpha=0.5, bins=30)\n",
    "axes[0].hist(survived.loc[survived[\"Sex\"] == \"female\"][\"Age\"].dropna(), label='Female', alpha=0.5, bins=30)\n",
    "\n",
    "axes[1].hist(died.loc[died[\"Sex\"] == \"male\"][\"Age\"].dropna(), label='Male', alpha=0.5, bins=30)\n",
    "axes[1].hist(died.loc[died[\"Sex\"] == \"female\"][\"Age\"].dropna(), label='Female', alpha=0.5, bins=30)\n",
    "\n",
    "axes[0].set_title('Survived')\n",
    "axes[1].set_title('Not Survived')\n",
    "\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[1].set_xlabel('Age')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of male passengers between 20 and 40 had bad luck, another interesting attribute fo focus on would\n",
    "be ``SibSp``, since it is most likley that male people of that age were travelling with some relatives\n",
    "(the number of siblings or spouses the passenger had aboard the Titanic).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sib = pd.pivot_table(df, index='SibSp', values='Survived')\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.plot(df_sib, marker='o')\n",
    "\n",
    "ax.set_xlabel(\"SibSp\")\n",
    "ax.set_title('Prob. of Survival')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As partially unveiled by the previous analysis, as the number the of siblings increase the probability of\n",
    "survival tends to decrease.\n",
    "\n",
    "Finally, since there may be some hidden pattern that we might have missed, we plot a factor matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(df,\n",
    "                 x_vars = ['Parch', 'Fare', 'SibSp'],\n",
    "                 y_vars = ['Parch', 'Fare', 'SibSp'],\n",
    "                 hue='Survived', \n",
    "                 diag_kind='kde', \n",
    "                 diag_kws={'alpha':0.5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "We can immediately see that females survived with much higher proportions than males did.\n",
    "\n",
    "Analougsly, the probability of survival increases as the ticket value increases.\n",
    "\n",
    "Another important features is the Age of the passengers. \n",
    "In general, youngsters have a higher probability of survival.\n",
    "\n",
    "Moreover, it seems that there is a sort of correlation between the probability of survival and the number\n",
    "of relative a person is travelling with.\n",
    "\n",
    "This first stage of evaluation also highlights some aspect about the data that need to be considered in the \n",
    "preprocessing stage.\n",
    "\n",
    "For instance:\n",
    "* There are missing values in the ``Age`` and ``Cabin`` column\n",
    "* The ``Age`` column is real-valued attributes. We should discretize it.\n",
    "* There are several columns with categorical attributes, we may want to represent them with one-hot-encoding\n",
    "\n",
    "---\n",
    "## Prepare the Data\n",
    "---\n",
    "__Suggestions__\n",
    "\n",
    "1. Write functions for all data transformations you apply, for five reasons:\n",
    "    * So you can easily prepare the data the next time you get a fresh dataset\n",
    "    * So you can apply these transformations in future projects\n",
    "    * To clean and prepare the test set\n",
    "    * To clean and prepare new data instances once your solution is live\n",
    "    * To make it easy to treat your preparation choices as hyperparameters\n",
    "2. Data cleaning:\n",
    "    * Fix or remove outliers (optional).\n",
    "    * Fill in missing values (e.g., with zero, mean, median...) or drop their rows (or columns).\n",
    "3. Feature selection (optional):\n",
    "    * Drop the attributes that provide no useful information for the task.\n",
    "4. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features.\n",
    "    * Decompose features (e.g., categorical, date/time, etc.).\n",
    "    * Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.).\n",
    "    * Aggregate features into promising new features.\n",
    "5. Feature scaling: standardize or normalize features.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with ``Cabin`` and ``Embarked``__\n",
    "\n",
    "Since ``Cabin`` has 77% of missing values it appears that the best way for dealing with this missing data is by\n",
    "dropping the entire column.\n",
    "\n",
    "Analougsly we can drop the few NaN instances in the corresponding to the ``Embarked`` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Cabin', axis=1)\n",
    "df = df.dropna(subset=['Embarked'], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with ``Age``__\n",
    "\n",
    "First, we will discretize the age column and then apply a one-hot-encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, Imputer, KBinsDiscretizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "age_pipeline = Pipeline([\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('discretize', KBinsDiscretizer(n_bins=5, encode='onehot'))\n",
    "    ])\n",
    "\n",
    "age_one_hot = age_pipeline.fit_transform(df[['Age']])\n",
    "age_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with ``PClass``__\n",
    "\n",
    "Again, the best way of representing this attribute would be by a one-hot-encoding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_pipeline = Pipeline([\n",
    "    ('discretize', OneHotEncoder())\n",
    "])\n",
    "\n",
    "pclass_one_hot = pclass_pipeline.fit_transform(df[['Pclass']])\n",
    "pclass_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Dealing with ``Name``, ``Sex``, ``Ticket``, ``Embarked``__\n",
    "\n",
    "These columns represent categorical values, it is better to have them as a one-hot-encoding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "cat_df = df.select_dtypes(['object'])\n",
    "cat_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the ``Ticket`` column does not have any value, we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Ticket', axis=1)\n",
    "cat_df = cat_df.drop('Ticket', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that also the ``Name`` column could bring some interesting information. \n",
    "\n",
    "In fact, Mr., Miss, Mrs., Maester, can denote the social class of an individual. This may have some degree of influence on\n",
    "the probaiblity of survival.\n",
    "\n",
    "Therefore, we will replace the original column with another one containing only the tile of a person (e.g., Mr., Mrs...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "names = cat_df.Name.str.split(' ')\n",
    "lnames = []\n",
    "for entry in names:\n",
    "    lnames.extend(entry)\n",
    "c = Counter(lnames)\n",
    "print(c.most_common()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common titles seems to be: Mr., Miss., and Master.\n",
    "\n",
    "We will use a regular expression for extracting this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "cat_df['Name'] = cat_df.Name.str.extract('(Mr|Miss|Mrs|Master)', flags=re.IGNORECASE).values\n",
    "if cat_df['Name'].isnull().sum() > 0:\n",
    "    cat_df['Name'] = cat_df['Name'].fillna('unk')\n",
    "assert cat_df.Name.isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a pipeline for transforming every categorical columns into a one-hot-encoding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no more missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline(\n",
    "    [('onehot', OneHotEncoder())]\n",
    ")\n",
    "sex_tick_emb_one_hot = cat_pipeline.fit_transform(cat_df)\n",
    "print(cat_df.columns,sex_tick_emb_one_hot.shape, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Finally, we can merge all these transformations in a single pipeline and then obtain the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf, ydf = df.loc[:, [\"Name\", \"Sex\", \"Embarked\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\"]], df.loc[:,[\"Survived\"]]\n",
    "# replace the name column\n",
    "Xdf['Name'] = cat_df['Name']\n",
    "Xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cat_attribs = list(cat_df.columns)\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "        (\"age_pipe\", age_pipeline, ['Age']),\n",
    "        (\"pclass_pipe\", pclass_pipeline, [\"Pclass\"])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "X, y = full_pipeline.fit_transform(Xdf), ydf.values.reshape(-1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Short-List Promising Models__\n",
    "__Suggestions__:\n",
    "\n",
    "1. If the data is huge, you may want to sample smaller training sets so you can train\n",
    "many different models in a reasonable time (be aware that this penalizes complex\n",
    "models such as large neural nets or Random Forests).\n",
    "Once again, try to automate these steps as much as possible.\n",
    "2. Train many quick and dirty models from different categories (e.g., linear, naive\n",
    "   Bayes, SVM, Random Forests, neural net, etc.) using standard parameters.\n",
    "3. Measure and compare their performance. \n",
    "   For each model, use N-fold cross-validation and compute the mean and stan‐\n",
    "   dard deviation of the performance measure on the N folds.\n",
    "4. Analyze the most significant variables for each algorithm.\n",
    "5. Analyze the types of errors the models make.\n",
    "   What data would a human have used to avoid these errors?\n",
    "6. Have a quick round of feature selection and engineering.\n",
    "7. Have one or two more quick iterations of the five previous steps.\n",
    "8. Short-list the top three to five most promising models, preferring models that\n",
    "   make different types of errors.   \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def compute_performance(estimators, X, y, scoring_funs, cross_val=True):\n",
    "    '''\n",
    "    Compute every score function for each estimator\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        estimators : a list of predictors\n",
    "        X : data used for computed the corss_validation_test\n",
    "        y : target values\n",
    "        scoring_fun : list of performances measures\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        A dictionary containing the score of each estimator\n",
    "    '''\n",
    "    score_dict = {}\n",
    "    for e in estimators:\n",
    "        score_dict[e.__class__.__name__] = {}\n",
    "        if not cross_val:\n",
    "            # we call predict on the estimator\n",
    "            y_pred = e.predict(X)\n",
    "\n",
    "        for f in scoring_funs: \n",
    "            if cross_val:\n",
    "                score = cross_val_score(e, X ,y, cv=10 ,scoring=\"accuracy\").mean()\n",
    "                name = f\n",
    "            else:\n",
    "                _f, name = f\n",
    "                score = _f(y_pred, y)\n",
    "            score_dict[e.__class__.__name__][name] = score\n",
    "    \n",
    "    return score_dict\n",
    "\n",
    "X_train, X_test,  y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we will try several classification algorithms.\n",
    "\n",
    "__SGDClassifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "compute_performance([sgd_clf], X_train, y_train, ['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NaiveBayes__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf = nb_clf.fit(X_train, y_train)\n",
    "compute_performance([nb_clf], X_train, y_train, ['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LogisticRegression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_clf = LogisticRegression(solver='lbfgs')\n",
    "logreg_clf.fit(X_train, y_train)\n",
    "compute_performance([logreg_clf], X_train, y_train, ['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Support Vector Machine__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(X_train, y_train)\n",
    "compute_performance([svc], X_train, y_train, ['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Fine-Tune the System__\n",
    "\n",
    "Notes:\n",
    "0. You will want to use as much data as possible for this step, especially as you move\n",
    "  toward the end of fine-tuning.\n",
    "1. Treat your data transformation choices as hyperparameters, especially when\n",
    "you are not sure about them (e.g., should I replace missing values with zero or\n",
    "with the median value? Or just drop the rows?).\n",
    "2. Unless there are very few hyperparameter values to explore, prefer random\n",
    "search over grid search. If training is very long, you may prefer a Bayesian\n",
    "optimization approach.\n",
    "3. Try Ensemble methods. Combining your best models will often perform better\n",
    "than running them individually.\n",
    "4. Once you are confident about your final model, measure its performance on the\n",
    "   test set to estimate the generalization error.\n",
    "---\n",
    "\n",
    "It appears that the last two models are the most promising ones.\n",
    "\n",
    "For this reason in the following section we are going to fine-tuning their parameter in\n",
    "order to conduct a more involved analysis.\n",
    "\n",
    "\n",
    "__Parameter Tuning for the LogisticRegression__\n",
    "\n",
    "We decide to tune the some of its parameter. More specifically: the ``solver`` and the ``penalty`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_logreg_grid = [\n",
    "    {'penalty': ['l2'], 'solver': ['newton-cg', 'sag','lbfgs']},\n",
    "  ]\n",
    "\n",
    "grid_search = GridSearchCV(logreg_clf, param_logreg_grid, cv=10,\n",
    "                           scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Param:\", grid_search.best_params_)\n",
    "best_logreg_clf = grid_search.best_estimator_\n",
    "compute_performance([best_logreg_clf], X_train, y_train, scoring_funs=['accuracy', 'recall','precision','f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Parameter Tuning for the SVC_\n",
    "\n",
    "We decide to try different kernel function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_svc_grid = [\n",
    "    {'kernel': ['linear', 'poly', 'rbf']},\n",
    "  ]\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_svc_grid, cv=10,\n",
    "                           scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Param:\", grid_search.best_params_)\n",
    "best_svc = grid_search.best_estimator_\n",
    "compute_performance([best_svc], X_train, y_train, scoring_funs=['accuracy', 'recall','precision','f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "From this analysis, it appears that the SVC is the best choice.\n",
    "\n",
    "## Ensemble Learning\n",
    "\n",
    "Before the ultimate deicsion about what is the best solution we want to try some ensemble technique.\n",
    "\n",
    "Since LogisticRegression behaves very well, we try a BaggingClassifier.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "                SVC(kernel='rbf'), #its best configuration \n",
    "                n_estimators=50,\n",
    "                bootstrap=True,\n",
    "                n_jobs=-1,\n",
    "                oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "compute_performance([bag_clf], X_train, y_train, scoring_funs=['accuracy', 'recall','precision','f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the BaggingClassifier is not able to boost the performance of the single learner.\n",
    "\n",
    "For this reason we try other ensamble methods.\n",
    "\n",
    "*Voting Classifier*\n",
    "\n",
    "Since LogisticRegression and SVC classifiers share similar performance, we can try to combine them\n",
    "in a VotingClassifier. \n",
    "\n",
    "We also include an additional DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "best_svc.probability = True\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', best_logreg_clf), ('svc', best_svc), ('dct', DecisionTreeClassifier())],\n",
    "    voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "compute_performance([voting_clf], X_train, y_train, scoring_funs=['accuracy', 'recall', 'precision', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are able to get a slightly boost in terms of accuracy and f1 score.\n",
    "\n",
    "\n",
    "*Random Forest*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(X,y)\n",
    "compute_performance([rnd_clf], X_train, y_train, scoring_funs=['accuracy', 'recall', 'precision', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest appears to behave worse than other simpler model. \n",
    "However we can use it for computing the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(rnd_clf.feature_importances_):\n",
    "    print(\"Position:{}: {}\".format(i,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "__Suggestion__:\n",
    "For a more exhaustive evaluation, you should try other ensamble learning methods (see Lecture 7a).\n",
    "However, if you get to this point, where none ensamble methods seems to guarantee a considerable boost\n",
    "in performance, you have basically two choices:\n",
    "\n",
    "   > (i) You settle for the best performance recorded so far, but you should also motivate why there was not any improvement\n",
    "\n",
    "   > (ii) You can reconsider some decisions taken earlier. \n",
    "    For instance, we neither normalized or standardize our dataset. \n",
    "    Another option would be to sample a subset of attributes from the original data, driven by the feature importance\n",
    "    compute by the RandomForestClassifier (be careful with the actual attributes they refer to, especially with One-Hot-Encoding\n",
    "\n",
    "Now, since I am the examiner and not the subject being examined, we will pretend the BaggingClassifier did an excellent job! :-)\n",
    "\n",
    "---\n",
    "---\n",
    "## Present Your Solution\n",
    "1. Document what you have done.\n",
    "2. Create a nice presentation.\n",
    "   Make sure you highlight the big picture first.\n",
    "3. Explain why your solution achieves the business objective.\n",
    "4. Don’t forget to present interesting points you noticed along the way. Describe what worked and what did not. List your assumptions and your system’s limitations.\n",
    "5. Ensure your key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., “the median income is the number-one    predictor of housing prices”\n",
    "\n",
    "---\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "Based on the previous analysis, three models have emerged as the most promising ones.\n",
    "\n",
    "In this section we are going to test every one of them against the test-set in order to confirm\n",
    "the results observed during the training stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = compute_performance([rnd_clf, voting_clf,bag_clf], X_test, y_test, scoring_funs=[(accuracy_score, \"accuracy\"), (f1_score, \"f1\")], cross_val=False)\n",
    "score_df = pd.DataFrame(score_dict)\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "__Note__:\n",
    "\n",
    "Here is a list of suggestions for improving the quality of this analysis.\n",
    "\n",
    "* You may want to remove some feature according to the random forest features importance\n",
    "* You may want to show the performance of each classifier wrt to other measure (e.g., the ROC curve)\n",
    "* If at the first try you don't succeed you can also report the failure and try to explain why that happens. \n",
    "  Actually, in this case we did not obtain considerable boost in performance due to the ensamble methods.\n",
    "  You could try to provide an explaination.\n",
    "  \n",
    "* Note also that the ``train_test_split`` should be called earlier than what we did. Also, it is better use pipeline for \n",
    "  automating the training episodes and the dataset transformation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
